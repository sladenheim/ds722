{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5732bf2",
   "metadata": {},
   "source": [
    "# Discussion Week 4\n",
    "\n",
    "In this discussion we review how to solve a least squares problem and how to solve linear systems of equations. \n",
    "\n",
    "You can use the Shared Computing Cluster (SCC) or Google Colab to run this notebook.\n",
    "\n",
    "The general instructions for running on the SCC are available under General Resources on [Piazza](https://piazza.com/bu/fall2025/ds722/resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c45804",
   "metadata": {},
   "source": [
    "## Problem: Least squares\n",
    "\n",
    "In this exercise, you'll generate a synthetic dataset with one independent variable $x$ and one dependent variable $y$, and fit a linear least squares model using `scikit-learn`. You could alternatively use `numpy` or `scipy` to solve the least squares problem directly. In class, we used `numpy.linalg.lstsq` to solve this problem.\n",
    "\n",
    "### Step 1: Generate Synthetic Data\n",
    "\n",
    "- Use `sklearn.datasets.make_regression` to create a dataset with:\n",
    "  - One feature (independent variable)\n",
    "  - One target (dependent variable)\n",
    "  - Add Gaussian noise to simulate measurement error\n",
    "\n",
    "The documentation for `make_regression` can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html).\n",
    "\n",
    "### Step 2: Fit a Linear Regression Model\n",
    "\n",
    "- Use `sklearn.linear_model.LinearRegression` to fit a least squares model.\n",
    "- Extract the slope (coefficient) and intercept.\n",
    "- Predict the outputs using the fitted model.\n",
    "\n",
    "The documentation for `LinearRegression` can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "### Step 3: Visualize the Fit\n",
    "\n",
    "- Plot the original data points \\((x, y)\\)\n",
    "- Overlay the fitted regression line\n",
    "\n",
    "### Step 4: Verify $b-Ax\\perp \\operatorname{Range}(A)$\n",
    "\n",
    "- Compute $r=b-Ax$ (be careful how you include the y-intercept)\n",
    "- Compute $A^{T}r$ and verify this is a vector of zeros (or near-zero numbers)\n",
    "- Compute the quantities $\\Vert Ax\\Vert_{2}^{2}$, $\\Vert r\\Vert_{2}^{2}$, and $\\Vert b\\Vert_{2}^{2}$ and numerically confirm that $\\Vert Ax\\Vert_{2}^{2} + \\Vert r\\Vert_{2}^{2} = \\Vert b\\Vert_{2}^{2}$\n",
    "- You will want to use the `numpy.linalg.norm` function to compute the 2-norm of a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Steps 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dd232",
   "metadata": {},
   "source": [
    "# Problem: LU factorization for $4\\times4$ matrices\n",
    "\n",
    "Using the `scipy.linalg.lu` function, compute the LU factorization of the following two matrices:\n",
    "\n",
    "$$\n",
    "A_{1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4 & 2 & 3 & 1 \\\\\n",
    "1 & 3 & 2 & 5 \\\\\n",
    "3 & 1 & 4 & 2 \\\\\n",
    "2 & 4 & 1 & 3 \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "A_{2}\n",
    "\\begin{bmatrix}\n",
    "2 & 1 & 3 & 4 \\\\\n",
    "4 & 2 & 6 & 8 \\\\\n",
    "6 & 0 & 9 & 12 \\\\\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The documentation for the `scipy.linalg.lu` function can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html). Observe that this function returns a permutation matrix $P$ as well as the matrices $L$ and $U$, such that $A = PLU$.\n",
    "\n",
    "Be sure to print out the matrices $P$, $L$ and $U$ for each case, and verify that $A=PLU$.\n",
    "\n",
    "For $A_{2}$, what do you notice about the rows of $U$? What does this imply about the invertibility of $A_{2}$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16343c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO A1 LU factorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37347251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO A2 LU factorization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds722",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
