---
title: "Midterm Review"
jupyter: python3
---

## Lecture Overview

- Vectors and Matrices (Lectures 1, 2)
- Orthogonality and Projections (Lecture 3)
- QR Factorization (Lecture 4)
- Least Squares (Lecture 5)
- Linear Systems (Lecture 6)
- Eigenvalues (Lecture 7)
- SVD (Lecture 8)
- Derivatives (Lecture 9, 10) and Integration (Lecture 11)

## Vectors and Matrices

Important to know:

- inner (dot) products and notation
- matrix-vector multiplication
- matrix-matrix multiplication
- transpose
- inverses
- angle between two vectors
- rank of a matrix
- norms

---

### Inner Product
Let $x,y\in\mathbb{R}^{n}$ then



---

### Matrix-Vector Multiplication
Let $A\in\mathbb{R}^{m\times n}$, $x\in\mathbb{R}^{n}$ and $b\in\mathbb{R}^{m}$ then

---

### Matrix-Matrix Multiplication
Let $A\in\mathbb{R}^{m\times n}$, $B\in\mathbb{R}^{n\times p}$, then $C=AB\in\mathbb{R}^{m\times p}$ and

---

**Example**

Find the angle $\theta$ (from its cosine) between the vectors $x = \begin{bmatrix} 1 \\ \sqrt{3}\end{bmatrix}$ and $y=\begin{bmatrix} 1 \\ 0 \end{bmatrix}$.

::: {.notes}
$$
\cos\theta=\frac{x\cdot y}{\|x\|\|y\|}
=\frac{1\cdot 1+\sqrt{3}\cdot 0}{\sqrt{1^2+(\sqrt{3})^2}\sqrt{1^2+0^2}}
=\frac{1}{\sqrt{4}\cdot 1}=\frac{1}{2}.
$$
Hence $\theta=\arccos\!\left(\tfrac{1}{2}\right)=\tfrac{\pi}{3}$.
:::

---

**Example**

Let
$$
A = \begin{bmatrix} 2 & -1 & 3 \\[4pt] 0 & 4 & -2 \\[4pt] 1 & 1 & 1 \end{bmatrix}, \qquad
x = \begin{bmatrix} 3 \\ -1 \\ 2 \end{bmatrix}.
$$

Compute the product $Ax$.

---

**Example**

If $A=X\Lambda X^{-1}$, compute $A^{k}$.

::: {.notes}
$$
A^{k} = (X\Lambda X^{-1})(X\Lambda X^{-1})\cdots(X\Lambda X^{-1})(X\Lambda X^{-1}) = X\Lambda^{k}X^{-1}.
$$
:::
---

**Example 3**
Assume that $A\in\mathbb{R}^{m\times m}$ has the factorization $A=LDU$. Show that if $A$ is symmetric ($A=A^{T}$), then $U=L^{T}$.

::: {.notes}
$$
A=LDU \qquad\Rightarrow\qquad A^T=U^T D L^T,
$$
and if $A=A^T$ then
$$
LDU=U^T D L^T.
$$
Assuming the usual normalization ( $L$ unit lower-triangular, $U$ unit upper-triangular, $D$ diagonal ), the LU factorization with this normalization is unique. Comparing the two factorizations of $A$ (one with factors $L,D,U$, the other with $U^T,D,L^T$) forces
$$
U^T=L\quad\text{and}\quad L^T=U,
$$
hence
$$
U=L^T.
$$
:::

## Orthogonality

Important to know:

- orthogonal vectors
- orthogonal matrices

---

**Example**

Determine whether the two vectors $x=\begin{bmatrix}1 \\ 2 \\ 3 \\ 4 \end{bmatrix}$ and $y=\begin{bmatrix}-1 \\ -2 \\ -4 \\ -3 \end{bmatrix}$ are orthogonal?


---

**Example**

Let
$$
Q=\begin{bmatrix}
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} & 0 \\
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
$$

Verify that $Q^{T}Q=I$ (equivalently, its columns are orthonormal). Compute $\det(Q)$ as an additional check.


## Projections

Important to know:

- definition of projection matrix
- definition of complementary projection
- orthogonal projectors

::: {.notes}
**Projection matrix.**
A matrix $P \in \mathbb{R}^{n\times n}$ is a projection if $P^2 = P$ (idempotent).
- Projection onto the column space of $A$ ($A \in \mathbb{R}^{n\times k}$, full column rank):
    $P = A (A^T A)^{-1} A^T$.
- In general an (oblique) projector maps $\mathbb{R}^n$ onto a subspace $S$ along some complementary subspace $N$; $\operatorname{range}(P)=S$ and $\operatorname{null}(P)=N$.

**Complementary projector.**
The complementary projector to $P$ is
    $P_c = I - P$.
It projects onto the complementary subspace (the subspace along which $P$ projects). Properties:
- $P + P_c = I$, $P P_c = 0$, $P_c^2 = P_c$.

**Orthogonal projector.**
A projector $P$ is orthogonal iff it is symmetric as well: $P = P^T$ and $P^2 = P$.
- If $Q \in \mathbb{R}^{n\times k}$ has orthonormal columns spanning $S$, the orthogonal projector onto $S$ is
    $P = Q Q^T$.
- For an orthogonal projector: $\operatorname{range}(P)=S$ and $\operatorname{null}(P)=S^\perp$. Orthogonal projectors minimize Euclidean distance to the subspace.
:::

---

**Example**

If $A\in\mathbb{R}^{m\times m}$ has full rank and $A^{2}=A$, prove that $A=I$.


## QR Factorization

Important to know:

- Definition of QR factorization (reduced and full)
- Gram-Schmidt orthogonalization
- Difference between classical and modified Gram-Schmidt algorithms

---

### CGS Algorithm
Let $A\in\mathbb{R}^{m\times n}$

1. for $j=1,\ldots, n$
1. $\quad\mathbf{v}_{j} = \mathbf{a}_{j}$
1. $\quad\quad$ for $i=1,\ldots, j-1$
1. $\quad\quad\quad r_{ij} = \mathbf{q}_{i}^{T}\mathbf{a}_{j}$ 
1. $\quad\quad\quad \mathbf{v}_{j} = \mathbf{v}_{j} - r_{ij}q_{i}$
1. $\quad\quad r_{jj} = \Vert \mathbf{v}_{j}\Vert_2$ 
1. $\quad\quad \mathbf{q}_{j} = \mathbf{v}_{j}/r_{jj}$ 


## Least Squares

Important to know:

- Normal equations
- Projections
- Orthogonality of residual

---

**Example**

Let
$$
A=\begin{bmatrix}
1 & 2 \\
0 & 1 \\
1 & -1
\end{bmatrix}, \qquad
b=\begin{bmatrix}
3 \\[4pt] 1 \\[4pt] 0
\end{bmatrix}.
$$

Compute the least squares solution $\hat{x}$ that minimizes $\|Ax-b\|_2$. Solve the normal equations $A^{T}A\hat{x}=A^{T}b$, and report $\hat{x}$, the residual $r=b-A\hat{x}$, and $\|r\|_2$.

---

**Solution**

::: {.notes}
Compute the normal equations: $A^T A \hat{x} = A^T b$.

$$
A^T A = \begin{bmatrix}2 & 1 \\[4pt] 1 & 6\end{bmatrix}, \qquad
A^T b = \begin{bmatrix}3 \\[4pt] 7\end{bmatrix}.
$$

Solve
$$
\begin{aligned}
2x_1 + x_2 &= 3,\\
x_1 + 6x_2 &= 7.
\end{aligned}
$$
From the first equation $x_2 = 3-2x_1$. Substituting gives $-11x_1 = -11$, so $x_1=1$ and $x_2=1$.

Therefore
$$
\hat{x} = \begin{bmatrix}1 \\[4pt] 1\end{bmatrix}, \qquad
r = b - A\hat{x} = \begin{bmatrix}0 \\[4pt] 0 \\[4pt] 0\end{bmatrix}, \qquad
\|r\|_2 = 0.
$$
:::


## Linear Systems

Important to know:

- How to solve a linear system of equations
- LU decomposition
- Pivoting


## Eigenvalues

Important to know:

- eigenvalue definition
- characteristic polynomial
- similarity
- diagonalization
- computing eigenvalues for 2x2 matrices

---

**Example**
Let $A = \begin{bmatrix}1 & 2 \\ 2 & 4 \end{bmatrix}$ compute the eigenvalues and eigenvectors.

::: {.notes}
$$
A=\begin{bmatrix}1 & 2\\[4pt] 2 & 4\end{bmatrix}
$$

Characteristic polynomial:
$$
\det(A-\lambda I)=\det\begin{bmatrix}1-\lambda & 2\\[4pt] 2 & 4-\lambda\end{bmatrix}=(1-\lambda)(4-\lambda)-4=\lambda(\lambda-5).
$$

Eigenvalues:
$$
\lambda_1=5,\qquad \lambda_2=0.
$$

Eigenvector for $\lambda_1=5$ (solve $(A-5I)x=0$):
$$
\begin{bmatrix}-4 & 2\\[4pt] 2 & -1\end{bmatrix}\begin{bmatrix}x_1\\[4pt] x_2\end{bmatrix}=0
\quad\Rightarrow\quad x\propto\begin{bmatrix}1\\[4pt] 2\end{bmatrix}.
$$

Eigenvector for $\lambda_2=0$ (solve $Ax=0$):
$$
\begin{bmatrix}1 & 2\\[4pt] 2 & 4\end{bmatrix}\begin{bmatrix}x_1\\[4pt] x_2\end{bmatrix}=0
\quad\Rightarrow\quad x\propto\begin{bmatrix}2\\[4pt] -1\end{bmatrix}.
$$

(Optional normalized eigenvectors)
$$
u_1=\frac{1}{\sqrt{5}}\begin{bmatrix}1\\[4pt] 2\end{bmatrix},\qquad
u_2=\frac{1}{\sqrt{5}}\begin{bmatrix}2\\[4pt] -1\end{bmatrix}.
$$
:::

## Singular Value Decomposition

Important to know:

- Definition of SVD (reduced vs. full)
- Geometric interpretation
- Computing SVD for 2x2 matrices


## Derivatives

Important to know:

- Derivatives and partial derivatives
- Derivatives w.r.t. vectors and matrices
- Automatic Differentiation
- Compute gradients and Hessians

---

**Example**
Draw the computational graph and use automatic differentiation to calculate the partial derivatives of $$f(x,y) = \cos(x^{2} + 3y) + \sqrt{y^{2} + 3x}$$

## Integration

Important to know:

- Simple integrals
- Definite vs. Indefinite integrals
- Fundamental theorem of calculus

---

**Example**

Compute $\int_{0}^{2}\int_{-1}^{1}x^{2}\frac{\cos{\pi y}}{\pi}dxdy$

---

**Example**

Compute $\int\int x^{2}\frac{\cos{\pi y}}{\pi}dxdy$

## Exam Advice

- Focus on core concepts and common problem types; know key definitions and when to apply them.
- At the exam: skim all questions, allocate time, and do easy problems first to secure points.
- Show clear steps and reasoning for partial credit.
- If stuck, try special cases, simpler versions, or write what you know to earn partial credit.
- Rest and hydrate before the test; avoid last-minute cramming that increases stress.

::: {.notes}
You've learned a lot so far. This is an attempt to show what you have learned!
:::