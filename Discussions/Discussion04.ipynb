{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862f3957",
   "metadata": {},
   "source": [
    "# Discussion 4\n",
    "\n",
    "In this discussion we review the SVD and least squares by focusing on their computational aspects. \n",
    "\n",
    "You can use the Shared Computing Cluster (SCC) or Google Colab to run this notebook.\n",
    "\n",
    "The general instructions for running on the SCC are available under General Resources on [Piazza](https://piazza.com/bu/fall2025/ds722/resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351af689",
   "metadata": {},
   "source": [
    "## SVD\n",
    "\n",
    "\n",
    "The **Singular Value Decomposition (SVD)** of a $2 \\times 2$ matrix $A$ factors it as:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $U$ is a $2 \\times 2$ orthogonal matrix (left singular vectors),\n",
    "- $\\Sigma$ is a $2 \\times 2$ diagonal matrix with non-negative real numbers (singular values),\n",
    "- $V^T$ is the transpose of a $2 \\times 2$ orthogonal matrix (right singular vectors).\n",
    "\n",
    "The procedure to compute the SVD by hand for these small-scale matrices (you do something else for $n\\times n$ when $n$ is large) is given below.\n",
    "\n",
    "### Step-by-Step Procedure\n",
    "\n",
    "Let $A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$\n",
    "\n",
    "#### 1. Compute $A^T A$\n",
    "\n",
    "$$\n",
    "A^T A = \\begin{bmatrix} a & c \\\\ b & d \\end{bmatrix} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} a^2 + c^2 & ab + cd \\\\ ab + cd & b^2 + d^2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This matrix is symmetric and positive semi-definite.\n",
    "\n",
    "#### 2. Find Eigenvalues of $A^T A$\n",
    "\n",
    "These are the squares of the singular values:\n",
    "- Solve the characteristic polynomial: $\\det(A^T A - \\lambda I) = 0$\n",
    "- Let the eigenvalues be $\\lambda_1, \\lambda_2$, then: $\\sigma_1 = \\sqrt{\\lambda_1}, \\quad \\sigma_2 = \\sqrt{\\lambda_2}$\n",
    "  (Assume $\\sigma_1 \\geq \\sigma_2 \\geq 0$)\n",
    "\n",
    "#### 3. Compute Right Singular Vectors (Columns of $V$)\n",
    "\n",
    "- For each eigenvalue $\\lambda_i$, solve: $(A^T A - \\lambda_i I)v_i = 0$\n",
    "- Normalize each eigenvector $v_i$ to get orthonormal columns of $V$\n",
    "\n",
    "#### 4. Compute Left Singular Vectors (Columns of $U$)\n",
    "\n",
    "- Use: $u_i = \\frac{1}{\\sigma_i} A v_i$\n",
    "- Normalize each $u_i$ to ensure orthonormality\n",
    "\n",
    "#### 5. Construct $\\Sigma$\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0c0b4",
   "metadata": {},
   "source": [
    "## Problem 1: Compute the SVD\n",
    "\n",
    "Compute the SVD by hand for the following matrices\n",
    "\n",
    "$$\n",
    "A_{1} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\end{bmatrix},\\quad A_{2} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Verify your calculation using the [NumPy SVD](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44ada5",
   "metadata": {},
   "source": [
    "## Problem 2: PCA vs SVD on the Iris Dataset\n",
    "\n",
    "In this exercise, you'll perform Principal Component Analysis (PCA) on the classic Iris dataset and compare the results with standard SVD.\n",
    "\n",
    "### Step 1: Load and Preprocess the Data\n",
    "\n",
    "- Load the Iris dataset using `sklearn.datasets.load_iris`.\n",
    "- Extract the feature matrix $ A \\in \\mathbb{R}^{150 \\times 4}$.\n",
    "- Center the data by subtracting the mean of each feature.\n",
    "\n",
    "### Step 2: Perform PCA\n",
    "\n",
    "- Perform PCA using `PCA`  from scikit-learn with `n_components=2`.\n",
    "- The PCA scores are the coordinates of your data in the feature space. Print the first 10 PCA scores. \n",
    "- Print the PCA explained variance.\n",
    "\n",
    "\n",
    "### Step 3: Perform SVD\n",
    "\n",
    "- Apply `numpy.linalg.svd` directly to the centered data matrix $A$.\n",
    "- Use the first two columns of $V$ (right singular vectors) to project the data. Print the first 10 entries of the projected data.\n",
    "- Compare the SVD-based projection with the PCA-based projection.\n",
    "\n",
    "### Step 4: Visualization\n",
    "\n",
    "- Plot the PCA and SVD projections side by side.\n",
    "- Color the points by their species labels.\n",
    "\n",
    "\n",
    "### Questions to Answer\n",
    "\n",
    "1. Do the PCA and SVD projections look similar? If they are different, explain why.\n",
    "1. What is the relationship between the PCA scores and factors of the SVD?\n",
    "1. What do the singular values represent in the context of PCA?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d1ea3",
   "metadata": {},
   "source": [
    "## Problem 3: Least squares\n",
    "\n",
    "In this exercise, you'll generate a synthetic dataset with one independent variable $x$ and one dependent variable $y$, and fit a linear least squares model using `scikit-learn`.\n",
    "\n",
    "### Step 1: Generate Synthetic Data\n",
    "\n",
    "- Use `sklearn.datasets.make_regression` to create a dataset with:\n",
    "  - One feature (independent variable)\n",
    "  - One target (dependent variable)\n",
    "  - Add Gaussian noise to simulate measurement error\n",
    "\n",
    "### Step 2: Fit a Linear Regression Model\n",
    "\n",
    "- Use `sklearn.linear_model.LinearRegression` to fit a least squares model.\n",
    "- Extract the slope (coefficient) and intercept.\n",
    "- Predict the outputs using the fitted model.\n",
    "\n",
    "### Step 3: Visualize the Fit\n",
    "\n",
    "- Plot the original data points \\((x, y)\\)\n",
    "- Overlay the fitted regression line\n",
    "\n",
    "### Step 4: Verify $b-Ax\\perp \\operatorname{Range}(A)$\n",
    "\n",
    "- Compute $r=b-Ax$ (be careful onw how you include the y-intercept)\n",
    "- Compute $A^{T}r$ and verify this is a vector of zeros (or near-zero numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3abfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds722",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
