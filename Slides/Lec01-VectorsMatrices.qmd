---
title: "Vectors and Matrices"
jupyter: python3
---

## Lecture Overview

Data is (often) stored in arrays.

Linear algebra is the theoretical framework that allows us to manipulate and understand these data objects.

Examples:

:::: {.incremental}
- 1-D array (vector)
- 2-D array (matrix)
- 3-D array (tensor)
::::

## Lecture Objectives

- Introduce notation for vectors and matrices
- Understand vector and matrix operations
- Understand forward propagation in neural networks

## Neural Networks

![](figures/NeuralNetwork.png){fig-align="center"}

## Notation

A column vector of length $m$, $\mathbf{x}\in\mathbb{R}^{m}$, is a 1-dimensional (1-D) array of real numbers

$$
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_m
\end{bmatrix}.
$$

The $i$th element of the vector is denoted $x_i$.

---

A matrix with $m$ rows and $n$ columns, $A\in\mathbb{R}^{m\times n}$ is a 2-D array of numbers 
$$
A = 
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} &a_{m2} & \cdots & a_{mn} \\
\end{bmatrix}.
$$

The element at row $i$ and column $j$ is denoted $a_{ij}$. The matrix is square when $m=n$. A column vector is matrix of size $m\times 1$.

---

The transpose, $A^{T}$, of a matrix $A\in\mathbb{R}^{m\times n}$ is the $n\times m$ matrix

$$
A^{T} =
\begin{bmatrix}
a_{11} & a_{21} & \cdots & a_{m1} \\
a_{12} & a_{22} & \cdots & a_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} &a_{2n} & \cdots & a_{nm} \\
\end{bmatrix}.
$$

The transpose of a column vector $\mathbf{x}$ is a row vector. A row vector is a matrix of size $1\times m$.

## Vectors

Given $\alpha, \beta\in\mathbb{R}$ and $\mathbf{x}, \mathbf{y}\in\mathbb{R}^{m}$, a linear combination is defined as

$$
\alpha \mathbf{x} + \beta\mathbf{y} =
\begin{bmatrix}
\alpha x_1 + \beta y_1\\
\alpha x_2 + \beta y_2 \\
\vdots \\
\alpha x_m + \beta y_m
\end{bmatrix}
.
$$

## Span

 Let $S = \{\mathbf{v}_{1}, \ldots, \mathbf{v}_{n}\}$, be a set of vectors where $\mathbf{v}_i\in\mathbb{R}^m$. The $\operatorname{span}(S)$ is the set of all linear combinations of vectors in $S$

 $$
\operatorname{span}(S) = \left\{ \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \cdots + \alpha_n \mathbf{v}_n \right\}, \quad \alpha_i\in\mathbb{R}.
 $$

## Geometric Interpretation

The sum of $\mathbf{u} = [1, 2]$ and $\mathbf{v} = [4, 1]$ is plotted in @fig-vector-addition. 
The sum $\mathbf{u} + \mathbf{v} = [5, 3]$ is obtained by placing the tip of one vector to the tail of the other vector. 

```{python}
#| label: fig-vector-addition
#| fig-cap: "Vector addition"
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = plt.gca()
u = np.array([1, 2])
v = np.array([4, 1])
w = np.array([5, 3])
V = np.array([u, v, w])
origin = np.array([[0, 0, 0], [0, 0, 0]])
plt.quiver(*origin, V[:, 0], V[:, 1], 
           color=['r', 'r', 'b'], 
           angles='xy', 
           scale_units='xy', 
           scale=1)
ax.set_xlim([-1, 6])
ax.set_ylim([-1, 4])
ax.text(1.3, 1.9, '$u$', size=16)
ax.text(4.3, 1.2, '$v$', size=16)
ax.text(5.3, 2.9, '$u+v$', size=16)
plt.plot([1, 5], [2, 3], 'g--')
plt.plot([4, 5], [1, 3], 'g--')
ax.grid()
plt.show()
```

---

```{python}
#| label: fig-vector-span
#| fig-cap: "Span of 2 vectors"
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define vectors u and v
u = np.array([1, 2, 0])
v = np.array([4, 1, 0])

# Create a grid of points for the plane
s = np.linspace(-1, 1, 100)
t = np.linspace(-1, 1, 100)
S, T = np.meshgrid(s, t)

# Calculate the points on the plane
plane_points = np.outer(S.flatten(), u) + np.outer(T.flatten(), v)
plane_points = plane_points.reshape(S.shape[0], S.shape[1], 3)

# Create a 3D plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Plot the plane using plot_surface for a more continuous look
ax.plot_surface(plane_points[:, :, 0], plane_points[:, :, 1], plane_points[:, :, 2], color='lightblue', alpha=0.3)

# Plot the vectors u and v
ax.quiver(0, 0, 0, u[0], u[1], u[2], color='blue', label='u')
ax.quiver(0, 0, 0, v[0], v[1], v[2], color='blue', label='v')

# Set labels
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

# Set limits
ax.set_xlim([-5, 5])
ax.set_ylim([-5, 5])
ax.set_zlim([-5, 5])

# Add legend
ax.legend()

# Show plot
plt.show()
```

## Vector Spaces

A vector space is a set $V$ such that for any 2 elements $\mathbf{u},\mathbf{v}\in V$, and any scalars, $\alpha$ and $\beta$, then $\alpha\mathbf{u} + \beta\mathbf{v}\in V$. 

In addition, a vector space must satisfy the following properties

:::: {.incremental}
1. $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$ (associativity).
1. $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ (commutativity).
1. There exists $\mathbf{0}\in V$ such that $\mathbf{v} + \mathbf{0} = \mathbf{v}$ for all $\mathbf{v}\in V$ (identity element).
1. For every $\mathbf{v}\in V$, there exists $-\mathbf{v}\in V$ such that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$ (inverse).
1. $\alpha(\beta\mathbf{v}) = (\alpha\beta)\mathbf{v}$
1. $1\mathbf{v} = \mathbf{v}$
1. $c(\mathbf{u} + \mathbf{v}) = \alpha\mathbf{u} + \beta\mathbf{v}$
1. $(\alpha + \beta)\mathbf{v} = \alpha\mathbf{v} + \beta\mathbf{v}$
::::

---

A set of vectors $F = \{ \mathbf{f}_1, \ldots, \mathbf{f}_n \}$ spans a vectors space $V$ if every vector in $\mathbf{v}\in V$ can be expressed as a linear combination of the elements in F, i.e.,

$$
\mathbf{v} = \sum_{i=1}^{n}\alpha_i \mathbf{f}_i.
$$

## Linear Independence

A set of $n$ vectors $S =\{\mathbf{v}_{1}, \ldots, \mathbf{v}_{n}\}$ is linearly dependent if there exists scalars $\alpha_1,\ldots, \alpha_n$ not all zero such that 
$$
\sum_{i=1}^{n} \alpha_i \mathbf{v}_i = 0.
$$

---

The vectors in the set $S= \{\mathbf{v}_{1}, \ldots, \mathbf{v}_{n}\}$ are linearly independent if they are not linearly dependent, i.e., the equation
$$
\alpha_1 \mathbf{v}_1 + \cdots + \alpha_n \mathbf{v}_n = 0,
$$
is only satisfied when $\alpha_i = 0$ for $i=1,\ldots,n$.

## Basis

A basis for a vector $V$ is a set of vectors $B = \{\mathbf{b}_1, \ldots, \mathbf{b}_n\}$ that spans $V$ and is linearly independent.

Let $e_{k}\in\mathbf{R}^{n}$ denote a vector with a 1 in the $k$-th entry and zero everywhere else, then

$$
\{\mathbf{e}_1, \ldots, \mathbf{e}_n\}
$$

is called the standard basis.

## Inner Products

The inner product of two columns vectors $\mathbf{x}, \mathbf{y}\in\mathbb{R}^m$ is the product of the transpose of $\mathbf{x}$ by $\mathbf{y}$

$$
\mathbf{x}^{T}\mathbf{y} = \sum_{i=1}^{m}x_iy_i.
$$

---

The Euclidean length of $\mathbf{x}$ is denoted $\Vert \mathbf{x} \Vert_2$ is the square root of the inner product of $\mathbf{x}$ with itself

$$
\Vert \mathbf{x} \Vert_2 = \sqrt{\mathbf{x}^{T}\mathbf{x}} = \left(\sum_{i=1}^{m}\vert x_i\vert^{2}\right)^{1/2}.
$$

The cosine of the angle $\theta$ between $\mathbf{x}$ and $\mathbf{y}$ can be expressed in terms of the inner product:

$$
\cos(\theta) = \frac{\mathbf{x}^{T}\mathbf{y}}{\Vert \mathbf{x}\Vert_2 \Vert \mathbf{y}\Vert_2}.
$$

## Single Neuron

![](figures/Neuron.png){fig-align="center"}


## A Matrix Times a Vector

Let $\mathbf{x}\in\mathbb{R}^{n}$ and let $A\in\mathbb{R}^{m\times n}$, then the matrix-vector product $\mathbf{b}=A\mathbf{x}\in\mathbb{R}^{m}$ is defined as

$$
b_{i} = \sum_{j=1}^{n}a_{ij}x_j, \quad i=1,\ldots,m.
$$

---

The map $\mathbf{x} \rightarrow A\mathbf{x}$ is linear, i.e.,

$$
\begin{align*}
A(\mathbf{x} + \mathbf{y}) &= A\mathbf{x} + A\mathbf{y} \\
A(\alpha \mathbf{x}) &= \alpha A \mathbf{x}
\end{align*}
$$

---

Let $a_j\in\mathbb{R}^{m}$ denote the $j$th column of $A$, then

$$
b = \sum_{j=1}^{n}x_j a_j.
$$

## Range  

The range of a matrix $A\in\mathbb{R}^{m\times n}$, range($A$), is the set of vectors that can be expressed as $A\mathbf{x}$ for some $\mathbf{x}$.

**Theorem:** $\operatorname{range}(A)$ is the vector space spanned by the columns of $A$.

The $\operatorname{range}(A)$ is alternatively referred to as the column space.

## Nullspace

The nullspace of $A\in\mathbb{R}^{m\times n}$ is the set of vectors $\mathbf{x}\in\mathbb{R}^{n}$ that satisfy $A\mathbf{x} = 0$, where $0$ is the $0$-vector in $\mathbb{R}^{m}$.

## Neural Networks

![](figures/NeuralNetwork.png){fig-align="center"}

## A Matrix Times a Matrix

Let $A\in\mathbb{R}^{\ell \times m}$, $C\in\mathbb{R}^{m \times n}$, then the matrix vector product $B = AC \in\mathbb{R}^{\ell \times n}$ is defined as

$$
b_{ij} = \sum_{k=1}^{m} a_{ik}c_{kj}.
$$

## Batches in Neural Networks

![](figures/NeuralNetwork.png){fig-align="center"}

## Outer Products

The outer product is a matrix-matrix product between an $m$-dimensional column vector $\mathbf{u}$ with an $n$-dimensional row vector $\mathbf{v}$, the result is an $m\times n$ (rank 1) matrix

$$
\begin{bmatrix}
u_1 \\
u_2 \\
\vdots \\
u_m
\end{bmatrix}
\begin{bmatrix}
v_1 & v_2 & \cdots & v_n
\end{bmatrix}
=
\begin{bmatrix}
v_1u_1 & \cdots & v_nu_1 \\
v_1u_2 & & v_nu_2 \\
\vdots & & \vdots \\
v_1u_m & \cdots & v_nu_m \\
\end{bmatrix}
$$

## Rank

The column rank of a matrix is the dimension of its column space.

The row rank of a matrix is the dimension of its row space.

**Theorem:** Row rank always equals column rank.

We refer to this single number as the rank of a matrix.

A matrix $A\in\mathbb{R}^{m\times n}$ of full rank is one that has the maximal possible rank, i.e., $\min(m, n)$.

## Determinant

The determinant of a square matrix $A\in\mathbb{R}^{n\times n}$ is

$$
det(A) = \sum_{j=1}^{n}(-1)^{i+j}a_{ij}M_{ij},
$$

where $M_{ij}=det(A_{ij})$ and $A_{ij}$ is an $n-1\times n-1$ matrix obtained by removing the $i$th row and the $j$th column.

## Inverse

A nonsingular or invertible matrix is a square matrix of full rank.

The inverse of an invertible matrix $A$ is denoted by $A^{-1}$ and satisfies $AA^{-1} = A^{-1}A = I$, where $I$ is the identity matrix.

---

Note that $(A^{T})^{-1} = A^{-T}$. Why?

**Theorem**

For $A\in\mathbb{R}^{n\times n}$ the following conditions are equivalent:

1. $A$ has an inverse $A^{-1}$
1. $\operatorname{rank}(A) = n$
1. $\operatorname{range}(A) = \mathbb{R}^{n}$
1. $\operatorname{null}(A) = {0}$
1. $0$ is not an eigenvalue of $A$
1. $0$ is not a singular value of $A$
1. $det(A) \neq 0$

## Vector Norms

The notions of size and distance in a vector space are captured by norms.

A norm is a function $\Vert\cdot\Vert: \mathbb{R}^{m} \rightarrow \mathbb{R}$ that assigns a length to each vector. This function satisfies for all vectors $\mathbf{x},\mathbf{y}$

1. $\Vert \mathbf{x} \Vert \geq 0$, and $\Vert \mathbf{x}\Vert = 0$ only if $\mathbf{x}=0$
1. $\Vert \mathbf{x} + \mathbf{y} \Vert \leq \Vert \mathbf{x} \Vert + \Vert\mathbf{y} \Vert$
1. $\Vert \alpha\mathbf{x} \Vert = \vert \alpha\vert \Vert\mathbf{x} \Vert$


## p-norms

The most important class of vectors norms are the p-norms

$$
\Vert \mathbf{x} \Vert_{p} = \left( \sum_{i=1}^{m}\vert x_i\vert^{p}\right)^{1/p}.
$$

---

When $p=2$ we have the Euclidean distance:

$$
\Vert \mathbf{x} \Vert_{2} = \left( \sum_{i=1}^{m}\vert x_i\vert^{2}\right)^{1/2}.
$$

--- 
```{python}
#| label: fig-2-norm
#| fig-cap: "Unit Ball under the 2-Norm"
import numpy as np
import matplotlib.pyplot as plt

# Create a grid of points
x = np.linspace(-1, 1, 400)
y = np.linspace(-1, 1, 400)
X, Y = np.meshgrid(x, y)

# Calculate the 2-norm for each point in the grid
p=2
norm_2 = (np.abs(X)**p + np.abs(Y)**p)**(1/p)

# Plot the unit ball under the 2-norm
plt.figure(figsize=(8, 8))
plt.contour(X, Y, norm_2, levels=[1], colors='red')
plt.title('Unit Ball under 1-Norm')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.axis('equal')
plt.show
```

---

When $p=1$ we have the Manhattan distance:

$$
\Vert \mathbf{x} \Vert_{1} = \sum_{i=1}^{m}\vert x_i\vert.
$$

---

```{python}
#| label: fig-1-norm
#| fig-cap: "Unit Ball under 1-Norm"
import numpy as np
import matplotlib.pyplot as plt

# Create a grid of points
x = np.linspace(-1, 1, 400)
y = np.linspace(-1, 1, 400)
X, Y = np.meshgrid(x, y)

# Calculate the 1-norm for each point in the grid
p=1
norm_1 = (np.abs(X)**p + np.abs(Y)**p)**(1/p)

# Plot the unit ball under the 1-norm
plt.figure(figsize=(8, 8))
plt.contour(X, Y, norm_1, levels=[1], colors='red')
plt.title('Unit Ball under 1-Norm')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.axis('equal')
plt.show
```

---

When $p\rightarrow\infty$ we have the infinity norm:
$$
\Vert \mathbf{x} \Vert_{\infty} = \max_{1\leq i \leq m} \vert x_i\vert.
$$

---

```{python}
#| label: fig-inf-norm
#| fig-cap: "Unit Ball under the Infinity-Norm"
import numpy as np
import matplotlib.pyplot as plt

# Create a grid of points
x = np.linspace(-1.5, 1.5, 400)
y = np.linspace(-1.5, 1.5, 400)
X, Y = np.meshgrid(x, y)

# Calculate the infinity-norm for each point in the grid
norm_inf = np.maximum(np.abs(X), np.abs(Y))

# Plot the unit ball under the infinity-norm
plt.figure(figsize=(8, 8))
plt.contour(X, Y, norm_inf, levels=[1], colors='red')
plt.title('Unit Ball under 1-Norm')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.axis('equal')
plt.show
```

## Matrix Norms Induced by Vector Norms

Let $A\in\mathbb{R}^{m\times n}$, the induced matrix norm is defined as

$$
\Vert A \Vert_{(m,n)} = \sup_{\mathbf{x}\in\mathbf{R}^{n}\\ \mathbf{x}\neq 0}\frac{\Vert A\mathbf{x}\Vert_{(m)}}{\Vert \mathbf{x}\Vert_{(n)}}.
$$

---

The following are commonly used matrix norms

1. $\Vert A \Vert_1 = \max_{1\leq j\leq n}\sum_{j=1}^{m} \vert a_{ij}\vert$
1. $\Vert A \Vert_2 = \sigma_{\text{max}}(A)$, where $\sigma_{\text{max}}(A)$ is the largest singular value ^[We will cover singular values in later lectures.].
1. $\Vert A \Vert_\infty = \max_{1\leq j\leq m}\sum_{j=1}^{n} \vert a_{ij}\vert$

---

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Define the matrix
A = np.array([[1, 2], [0, 2]])

# Define the unit circle
theta = np.linspace(0, 2 * np.pi, 100)
unit_circle = np.array([np.cos(theta), np.sin(theta)])

# Transform the unit circle using the matrix A
transformed_circle = A @ unit_circle

# Find the vector on the unit circle that is amplified the most
u, s, vh = np.linalg.svd(A)
max_vector = vh[0]

# Transform the max vector
transformed_max_vector = A @ max_vector

# Plot the original unit circle
plt.plot(unit_circle[0], unit_circle[1], 'b--', label='Unit Circle')

# Plot the transformed unit circle
plt.plot(transformed_circle[0], transformed_circle[1], 'r-', label='Transformed Unit Circle')

# Plot the max vector and its transformed image
plt.plot([0, max_vector[0]], [0, max_vector[1]], 'g-', label='Max Vector')
plt.plot([0, transformed_max_vector[0]], [0, transformed_max_vector[1]], 'k-', label='Transformed Max Vector')

# Add labels and legend
plt.xlabel('x')
plt.ylabel('y')
plt.axhline(0, color='black',linewidth=0.5)
plt.axvline(0, color='black',linewidth=0.5)
plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)
plt.legend()
plt.title('Image of the Unit Ball under the Matrix 2-Norm')
plt.axis('equal')

# Show the plot
plt.show()
```

## Frobenius Norm

The Frobenius norm is defined as

$$
\Vert A \Vert_F = \left(\sum_{j=1}^{n}\sum_{i=1}^{m}\vert a_{ij}\vert^{2}\right)^{1/2} = \left(\sum_{j=1}^{n}\Vert \mathbf{a}_{j}\Vert_{2}^{2}\right)^{1/2}.
$$

The trace operator, $\operatorname{tr}(A) = \sum_{i=1}^{m}a_{ii}$


$$
\Vert A \Vert_F = \sqrt{\operatorname{tr}(A^{T}A)} = \sqrt{\operatorname{tr}(AA^{T})}.
$$

## Cauchy-Schwarz and H&ouml;lder Inequalities

Let $p$ and $q$ satisfy $\frac{1}{p} + \frac{1}{q} = 1$, with $1\leq p,q\leq \infty$, then the H&ouml;lder inequality states that for any vectors $\mathbf{x}$ and $\mathbf{y}$

$$
\vert x^{T}y\vert \leq \Vert \mathbf{x}\Vert_{p}\Vert\mathbf{y}\Vert_{q}.
$$

The Cauchy-Schwarz inequality is a special case when $p=q=2$:

$$
\vert x^{T}y\vert \leq \Vert \mathbf{x}\Vert_{2}\Vert\mathbf{y}\Vert_{2}.
$$

## Complex Numbers

As data scientists we predominantly work with the set of real numbers $\mathbb{R}$.

However, the theoretical results that we discuss in this course work also apply for the set of complex numbers $\mathbb{C}$, i.e., numbers of the form $z=a+ib$, where $i=\sqrt{-1}$.

The conjugate of a complex number is $\bar{z} = a-ib$. For real numbers $\bar{z}=z$.

For vectors and matrices with complex numbers, we use the $^{*}$ notation to indicate the Hermitian conjugate, i.e., 

$$
A = 
\begin{bmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}\\
a_{31} & a_{32}\\
\end{bmatrix} 
\Rightarrow
A^{*} = 
\begin{bmatrix}
\bar{a}_{11} & \bar{a}_{21} & \bar{a}_{31}\\
\bar{a}_{11} & \bar{a}_{22} & \bar{a}_{32}\\
\end{bmatrix}
$$

## Summary

We introduced:

:::: {.incremental}
1. Key notation and terminology for vectors and matrices
1. Vector spaces, Span, and Basis 
1. Matrix-vector and matrix-matrix products
1. How a neural network uses matrix operations for forward propagation
1. Vector and matrix norms
::::
