{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6592ac40",
   "metadata": {},
   "source": [
    "# Discussion Week 6\n",
    "\n",
    "In this discussion we review the Derivative and integrals by focusing on their computational aspects. \n",
    "\n",
    "You can use the Shared Computing Cluster (SCC) or Google Colab to run this notebook.\n",
    "\n",
    "The general instructions for running on the SCC are available under General Resources on [Piazza](https://piazza.com/bu/fall2025/ds722/resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a69dcd",
   "metadata": {},
   "source": [
    "## Problem 1: Automatic Differentiation with PyTorch\n",
    "\n",
    "In this exercise, you'll explore how PyTorch builds computational graphs and computes gradients automatically using `autograd`.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Define scalar and vector functions using PyTorch tensors.\n",
    "- Use `.backward()` to compute gradients.\n",
    "- Visualize the computational graph and understand gradient propagation.\n",
    "\n",
    "### Step 1: Import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2469fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cfe8c",
   "metadata": {},
   "source": [
    "### Step 2: Scalar Function\n",
    "\n",
    "Below we illustrate how to use Pytorch autograd to compute $f(2)$, and $f^{\\prime}(2)$ of $f(x)=x^{2} + 3x + 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cc11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with requires_grad=True to track computation\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "f = x**2 + 3*x + 2\n",
    "\n",
    "# Compute the gradient\n",
    "f.backward()\n",
    "\n",
    "# Print the gradient df/dx\n",
    "print(\"x =\", x.item())\n",
    "print(\"f(x) =\", f.item())\n",
    "print(\"df/dx =\", x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca245e",
   "metadata": {},
   "source": [
    "### Step 3: Computational Graph\n",
    "\n",
    "Each operation in $f(x)$ on the tensor $x=2$ creates a node in the computational graph:\n",
    "\n",
    "- $c = 3\\cdot x$ (Multiplication)\n",
    "- $b = x\\cdot x$ (Power)\n",
    "- $a = b + c$ (Add)\n",
    "- $f = a + 2$ (Add)\n",
    "\n",
    "Using the `grad_fn` and `next_function` attributes, we can see the inner workings of the Pytorch data structures responsible for storing and calculating the partial derivatives to calculate the gradients.\n",
    "\n",
    "It is an efficient way to compute $\\nabla f(x)$ by first computing\n",
    "\n",
    "1. $df/da$\n",
    "1. $da/db$ and $da/dc$\n",
    "1. $db/dx$ and $dc/dx$\n",
    "\n",
    "then $\\nabla f(x) = (df/da)(da/db)(db/dx) + (df/da)(da/dc)(dc/dx)$.\n",
    "\n",
    "\n",
    "Execute the following cell to observe this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AddBackward object for adding the two quantities f = a + 2\n",
    "print(\"f.grad_fn:\", f.grad_fn)\n",
    "# AddBackward object for adding the two quantities a = b + c and None for the scalar 2\n",
    "print(\"f.grad_fn.next_functions:\", f.grad_fn.next_functions)\n",
    "# PowBackward object for b = x**2 and MulBackward for c=3*x\n",
    "print(\"f.grad_fn.next_functions:\", f.grad_fn.next_functions[0][0].next_functions)\n",
    "# AccumulateGrad object is a special node to tell the program  to store the calculated gradient of x when backward is called\n",
    "# Leaf node corresponding to the tensor x\n",
    "print(\"f.grad_fn.next_functions:\", f.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "# AccumulateGrad object is a placeholder for x and None for scalar 3\n",
    "print(\"f.grad_fn.next_functions:\", f.grad_fn.next_functions[0][0].next_functions[1][0].next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e0da2",
   "metadata": {},
   "source": [
    "### Step 4: Vector Function\n",
    "\n",
    "Create similar code as in Step 2 for the vector function $f(x,y)=x^{2}y + \\sin{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fda8a2",
   "metadata": {},
   "source": [
    "### Step 5: Computational Graph\n",
    "\n",
    "Create similar code as in Step 3 for the vector function $f(x,y)=x^{2}y + \\sin{y}$.\n",
    "\n",
    "Is it becoming clearer how Pytorch computes the gradients of functions and how autograd works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f475b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3db4c6",
   "metadata": {},
   "source": [
    "## Problem 2: Differentiating Tensor Expressions with Pytorch\n",
    "\n",
    "Recall from class when we computed the derivative $\\frac{dK}{dR}$ of $K=R^{T}R$ where $R\\in\\mathbb{R}\\in\\mathbb{R}^{m\\times n}$.\n",
    "\n",
    "The derivative $\\frac{dK}{dR}$ was a 4th order tensor of size $n\\times n\\times m\\times n$ and was given by the formula\n",
    "\n",
    "$$\n",
    "\\frac{\\partial K_{pq}}{\\partial R_{ij}} =\n",
    "\\begin{cases}\n",
    "R_{iq} & \\text{if}~j=p,~p\\neq q \\\\\n",
    "R_{ip} & \\text{if}~j=q,~p\\neq q \\\\\n",
    "R_{iq} & \\text{if}~j=p,~p=q \\\\\n",
    "0 & \\text{otherwise} \\\\\n",
    "\\end{cases},\n",
    "$$\n",
    "\n",
    "where $p,q,j=1,\\ldots,n$ and $i=1,\\ldots m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811603d8",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Complete the coding cell below that takes as input a random Pytorch tensor $R\\in\\mathbb{R}^{4\\times 3}$ and using the above formula, create the 4-D tensor $\\frac{\\partial K_{pq}}{\\partial R_{ij}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Dimensions\n",
    "m, n = 4, 3\n",
    "R = torch.randn(m, n, requires_grad=True)\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f92d7",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "\n",
    "Define a function `compute_K(R)` which computes the matrix multiplication $R^{T}R$. Then, using `torch.autograd.function.jacobian(compute_K, R)` return the tensor $\\frac{\\partial K_{pq}}{\\partial R_{ij}}$. Compare this with what you computed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8acaaa",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Let $f(x) = xx^{T}$ for $x\\in\\mathbb{R}^{n}$. Compute by hand the derivative $\\frac{df}{dx}$, then complete the code cell below to compute this for a random 1-D torch when $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae02b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b2a01",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Define a function `compute_f(x)` which computes the outer product $xx^{T}$. Then, using `torch.autograd.function.jacobian(f, x)` return the tensor $\\frac{\\partial f_{ij}}{\\partial x_{k}}$. Compare this with what you computed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds722",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
