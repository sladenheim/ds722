{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6592ac40",
   "metadata": {},
   "source": [
    "# Discussion 5\n",
    "\n",
    "In this discussion we review the Derivative and integrals by focusing on their computational aspects. \n",
    "\n",
    "You can use the Shared Computing Cluster (SCC) or Google Colab to run this notebook.\n",
    "\n",
    "The general instructions for running on the SCC are available under General Resources on [Piazza](https://piazza.com/bu/fall2025/ds722/resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a69dcd",
   "metadata": {},
   "source": [
    "## Problem 1: Automatic Differentiation with PyTorch\n",
    "\n",
    "In this exercise, you'll explore how PyTorch builds computational graphs and computes gradients automatically using `autograd`.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Define scalar and vector functions using PyTorch tensors.\n",
    "- Use `.backward()` to compute gradients.\n",
    "- Visualize the computational graph and understand gradient propagation.\n",
    "\n",
    "### Step 1: Import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2469fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cfe8c",
   "metadata": {},
   "source": [
    "### Step 2: Scalar Function\n",
    "\n",
    "Below we illustrate how to use Pytorch autograd to compute $f(2)$, and $f^{\\prime}(2)$ of $f(x)=x^{2} + 3x + 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cc11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with requires_grad=True to track computation\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "f = x**2 + 3*x + 2\n",
    "\n",
    "# Compute the gradient\n",
    "f.backward()\n",
    "\n",
    "# Print the gradient df/dx\n",
    "print(\"x =\", x.item())\n",
    "print(\"f(x) =\", f.item())\n",
    "print(\"df/dx =\", x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca245e",
   "metadata": {},
   "source": [
    "### Step 3: Computational Graph\n",
    "\n",
    "Each operation in $f(x)$ on the tensor $x=2$ creates a node in the computational graph:\n",
    "\n",
    "- $c = 3\\cdot x$ (Multiplication)\n",
    "- $b = x\\cdot x$ (Power)\n",
    "- $a = b + c$ (Add)\n",
    "- $f = a + 2$ (Add)\n",
    "\n",
    "Using the `grad_fn` and `next_function` attributes, we can see the inner workings of the Pytorch data structures responsible for storing and calculating the partial derivatives to calculate the gradients.\n",
    "\n",
    "It is an efficient way to compute $\\nabla f(x)$ by first computing\n",
    "\n",
    "1. $df/da$\n",
    "1. $da/db$ and $da/dc$\n",
    "1. $db/dx$ and $dc/dx$\n",
    "\n",
    "then $\\nabla f(x) = (df/da)(da/db)(db/dx) + (df/da)(da/dc)(dc/dx)$.\n",
    "\n",
    "\n",
    "Execute the following cell to observe this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AddBackward object for adding the two quantities f = a + 2\n",
    "print(\"f.grad_fn:\", f.grad_fn)\n",
    "# AddBackward object for adding the two quantities a = b + c and None for the scalar 2\n",
    "print(\"f.grad_fn.next_functions:\", f.grad_fn.next_functions)\n",
    "# PowBackward object for b = x**2 and MulBackward for c=3*x\n",
    "print(\"f.grad_fn.next_functions:\", f.grad_fn.next_functions[0][0].next_functions)\n",
    "# AccumulateGrad object is a special node to tell the program  to store the calculated gradient of x when backward is called\n",
    "# Leaf node corresponding to the tensor x\n",
    "print(\"f.grad_fn.next_functions:\", f.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "# AccumulateGrad object is a placeholder for x and None for scalar 3\n",
    "print(\"f.grad_fn.next_functions:\", f.grad_fn.next_functions[0][0].next_functions[1][0].next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e0da2",
   "metadata": {},
   "source": [
    "### Step 4: Vector Function\n",
    "\n",
    "Create similar code as in Step 2 for the vector function $f(x,y)=x^{2}y + \\sin{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fda8a2",
   "metadata": {},
   "source": [
    "### Step 5: Computational Graph\n",
    "\n",
    "Create similar code as in Step 3 for the vector function $f(x,y)=x^{2}y + \\sin{y}$.\n",
    "\n",
    "Is it becoming clearer how Pytorch computes the gradients of functions and how autograd works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f475b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824e3ad",
   "metadata": {},
   "source": [
    "## Problem 2: Numerical Integration\n",
    "\n",
    "In this problem we investigate two methods for numerical integration:\n",
    "\n",
    "- the trapezoid rule\n",
    "- Simpson's rule\n",
    "\n",
    "\n",
    "## Trapezoid Rule\n",
    "\n",
    "The Trapezoid Rule approximates the area under the curve using trapezoids:\n",
    "\n",
    "$$\n",
    "\\int_a^b f(x)\\,dx \\approx \\frac{h}{2} \\left[ f(x_0) + 2 \\sum_{i=1}^{n-1} f(x_i) + f(x_n) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ h = \\frac{b - a}{n} $\n",
    "- $ x_i = a + i h $ for $ i = 0, 1, \\dots, n $\n",
    "\n",
    "## Midpoint Rule\n",
    "\n",
    "The Midpoint Rule approximates the definite integral by evaluating the function at the midpoint of each subinterval:\n",
    "\n",
    "$$\n",
    "\\int_a^b f(x)\\,dx \\approx h \\sum_{i=0}^{n-1} f\\left(x_i + \\frac{h}{2}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ h = \\frac{b - a}{n} $ is the width of each subinterval\n",
    "- $ x_i = a + i h $ is the start of the $i$-th subinterval\n",
    "\n",
    "Your job is to\n",
    "\n",
    "1. Implement these rules as Python functions. \n",
    "1. Use your Python functions to approximate the integrals of\n",
    "    - $f_{1}(x) = \\sin{x}$ on $[0, \\pi]$\n",
    "    - $f_{2}(x) = x^{5}$ on $[0, 1]$\n",
    "1. Compute the error between your approximations and the true value of the integral as you double the number of points in your approximation. Plot the errors on a loglog plot. What does the slope of the line tell you about the error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2187d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Trapezoid Rule Implementation\n",
    "# -------------------------------\n",
    "def trapezoid_rule(f, a, b, n):\n",
    "    h = (b - a) / n\n",
    "    x = np.linspace(a, b, n + 1)\n",
    "    return h * (0.5 * f(x[0]) + np.sum(f(x[1:-1])) + 0.5 * f(x[-1]))\n",
    "\n",
    "# -------------------------------\n",
    "# Simpson's Rule Implementation\n",
    "# -------------------------------\n",
    "def simpsons_rule(f, a, b, n):\n",
    "    if n % 2 != 0:\n",
    "        raise ValueError(\"Simpson's rule requires an even number of intervals.\")\n",
    "    h = (b - a) / n\n",
    "    x = np.linspace(a, b, n + 1)\n",
    "    fx = f(x)\n",
    "    return (h / 3) * (fx[0] + fx[-1] + 2 * np.sum(fx[2:n:2]) + 4 * np.sum(fx[1:n:2]))\n",
    "\n",
    "# -------------------------------\n",
    "# Functions to Integrate\n",
    "# -------------------------------\n",
    "def f1(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def f2(x):\n",
    "    return x**5\n",
    "\n",
    "# Exact integrals\n",
    "exact_f1 = 2.0          \n",
    "exact_f2 = 1.0 / 6      \n",
    "\n",
    "# -------------------------------\n",
    "# Error Analysis\n",
    "# -------------------------------\n",
    "ns = [4, 8, 16, 32, 64, 128]\n",
    "errors_f1_trap = []\n",
    "errors_f1_simp = []\n",
    "errors_f2_trap = []\n",
    "errors_f2_simp = []\n",
    "\n",
    "for n in ns:\n",
    "    # f1: sin(x) on [0, pi]\n",
    "    trap_f1 = trapezoid_rule(f1, 0, np.pi, n)\n",
    "    simp_f1 = simpsons_rule(f1, 0, np.pi, n)\n",
    "    errors_f1_trap.append(abs(trap_f1 - exact_f1))\n",
    "    errors_f1_simp.append(abs(simp_f1 - exact_f1))\n",
    "    \n",
    "    # f2: x^5 on [0, 1]\n",
    "    trap_f2 = trapezoid_rule(f2, 0, 1, n)\n",
    "    simp_f2 = simpsons_rule(f2, 0, 1, n)\n",
    "    errors_f2_trap.append(abs(trap_f2 - exact_f2))\n",
    "    errors_f2_simp.append(abs(simp_f2 - exact_f2))\n",
    "\n",
    "# -------------------------------\n",
    "# Plotting Error Comparison\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.loglog(ns, errors_f1_trap, 's-', label='Trapezoid Rule')\n",
    "plt.loglog(ns, errors_f1_simp, 'o-', label=\"Simpson's Rule\")\n",
    "plt.title(r'Error for $f_1(x) = \\sin(x)$')\n",
    "plt.xlabel('Number of Subintervals (n)')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.loglog(ns, errors_f2_trap, 's-', label='Trapezoid Rule')\n",
    "plt.loglog(ns, errors_f2_simp, 'o-', label=\"Simpson's Rule\")\n",
    "plt.title(r'Error for $f_2(x) = x^5$')\n",
    "plt.xlabel('Number of Subintervals (n)')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda3999",
   "metadata": {},
   "source": [
    "3. The slope of the line is the order of convergence. For every doubling of points, the error goes down by a factor of 2 in the trapezoid rule. For Simpson's rule, when you double the number of points, the error decreases by a factor of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01fae34",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds722",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
