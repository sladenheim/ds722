{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e771a7",
   "metadata": {},
   "source": [
    "# Discussion Week 8\n",
    "\n",
    "In this discussion we review optimization by focusing on their computational aspects. \n",
    "\n",
    "You can use the Shared Computing Cluster (SCC) or Google Colab to run this notebook.\n",
    "\n",
    "The general instructions for running on the SCC are available under General Resources on [Piazza](https://piazza.com/bu/fall2025/ds722/resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee1139",
   "metadata": {},
   "source": [
    "## Problem: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df41efd",
   "metadata": {},
   "source": [
    "Consider the objective function\n",
    "\n",
    "$$\n",
    "f(x)=\\ln(e^{2x}+e^{-2x})\n",
    "$$\n",
    "\n",
    "This objective function is plotted in the following cell. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc924ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function f(x) = ln(e^(2x) + e^(-2x))\n",
    "def f(x):\n",
    "    return np.log(np.exp(2*x) + np.exp(-2*x))\n",
    "\n",
    "# Generate x values in the range [-2, 2]\n",
    "x = np.linspace(-2, 2, 400)\n",
    "y = f(x)\n",
    "\n",
    "# Plot the function\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Plot of $f(x) = \\\\ln(e^{2x} + e^{-2x})$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5406f",
   "metadata": {},
   "source": [
    "\n",
    "For this problem we wish to investigate the performance of the following optimization methods:\n",
    "\n",
    "- Gradient descent: $x_{k+1} = x_{k} - \\alpha_{k}\\nabla f_{k}$\n",
    "- Newton's method: $x_{k+1} = x_{k} - \\alpha_{k}\\nabla^{2}f_{k}^{-1} \\nabla f_{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b461c47",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Compute the gradient of this function by hand and determine the minimum solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30245a9",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Compute the Hessian of this function by hand and confirm it is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435faea",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Write Python functions for the objective function $f(x)$, the gradient function $\\nabla f(x)$, and the Hessian $\\nabla^{2} f(x)$. You will need to compute the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec785423",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Write a function `gradient_descent(x0, alpha, k)` which performs $k$ steps of the gradient descent method with learning rate $\\alpha$ and initial step $x_{0}$. The function should return the final iterate $x_{k}$ and the objective function value $f(x_{k})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63241878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f126459",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Write a function `newton_method(x0, alpha, k)` which performs $k$ steps of Newton's method with learning rate $\\alpha$ and initial step $x_{0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131730fc",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "Let $x_{0}=0.5$. Run 10 steps of gradient descent and Newton's method with $\\alpha_{k}=0.1$. Display a table of the values $x_{k}$ and the errors, i.e, $\\vert f(x_{k}) - f(x^{*})\\vert$ for each $k=1,\\ldots, 10$ and for both methods. Which method converges to the true solution faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1caeff5",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "\n",
    "Let $x_{0}=1.2$. Run 10 steps of gradient descent and Newton's method with $\\alpha_{k}=0.1$. Display a table of the values $x_{k}$ and the errors, i.e, $\\vert f(x_{k}) - f(x^{*})\\vert$ for each $k=1,\\ldots, 10$ and for both methods. Do both methods converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds722",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
