---
title: "Linear Systems and LU Factorization"
jupyter: python3
---

## Lecture Overview

Statistics and data science are concerned with data. 

The link between sample spaces and events to data is through the concept of a *random variable*.

Random variables will allow us to formulate distributions.

## Lecture Objectives

Understand:

- definition and examples of random variables
- probability mass and density functions (pmf, pdf)
- important distributions

## Random Variable

Let $\Omega$ be a sampel space. A *random variable* is a mapping 

$$
X: \Omega \rightarrow \mathbb{R},
$$

that assigns a real number $X(\omega)$ to each outcome $\omega$

---

Flip a coin ten times. Let $X(\omega)$ be the number of heads in the sequence $\omega$. If 

$$
\omega = HTHTHTHTHT
$$
then $X(\omega)=5$.

---

Let $\Omega = \{(x, y): x^{2} + y^{2} \leq 1\}$ be the unit disk. Assume we have a way to randomly sample this space, i.e., $\omega = (x, y)$. Then the following are random variables:

- $X(\omega) = x$
- $Y(\omega) = y$
- $Z(\omega) = x+y$
- $W(\omega) = \sqrt{x^{2+y^{2}}}$

---

Given a random variable $X$ and a subset $A$ of the real line, we define $X^{-1}(A) = \{\omega\in\Omega : X(\omega)\in A\}$ and let

$$
\begin{align}
\mathbb{P}(X\in A) &= \mathbb{P}(X^{-1}(A)) = \mathbb{P}(\{\omega\in\Omega : X(\omega)\in A\}) \\
\mathbb{P}(X = x) &= \mathbb{P}(X^{-1}(x)) = \mathbb{P}(\{\omega\in\Omega : X(\omega)=x\}) \\
\end{align}
$$

---

Flip a coin twice and let $X$ be the number of heads. Let's fill in the following two tables together

:::: {.columns}
::: {.column width="60%"}
| $\omega$ | $\mathbb{P}(\omega)$ | $X(\omega)$ |
|----------|----------------------|-------------|
:::
::: {.column width="40%"}
| $x$ | $\mathbb{P}(X=x)$ | 
|----------|--------------|
:::
::::


## Distribution Functions and Probability Functions

### Cumulative Distribution Functions

Given a random variable $X$, the *cumulative distribution function* (CDF) is the function

$$
F_{X}: \mathbb{R}\rightarrow [0, 1] 
$$
defined by

$$
F_{X}(x) = \mathbb{P}(X\leq x).
$$

The CDF contatins all the information about the random variable.

---

Flip a fair coin twice and let $X$ be the number of heads. Then $\mathbb{P}(X=0) = \mathbb{P}(X=2) = 1/4$ and $\mathbb{P}(X=1) = 1/2$, the CDF is

$$
F_{X}(x) = 
\begin{cases}
0 & x < 0 \\
1/4 & 0\leq x < 1 \\
3/4 & 1\leq x < 2 \\
1 & x\geq 2.
\end{cases}
$$

---

```{python}
import matplotlib.pyplot as plt
import numpy as np

x0 = np.linspace(-1, 0, 50)
y0 = 0*x0

x1 = np.linspace(0, 1, 50 )
y1 = 0.25*np.ones(len(x1))

x2 = np.linspace(1, 2, 50)
y2 = 0.75*np.ones(len(x1))

x3 = np.linspace(2, 3, 50)
y3 = np.ones(len(x3))

plt.plot(x0, y0, color='blue')
plt.plot(x1, y1, color='blue')
plt.plot(x2, y2, color='blue')
plt.plot(x3, y3, color='blue')

plt.plot(0, 0, 'o', markerfacecolor='white', markeredgecolor='blue')
plt.plot(0, 0.25, 'o', color='blue')  

plt.plot(1, 0.25, 'o', markerfacecolor='white', markeredgecolor='blue')
plt.plot(1, 0.75, 'o', color='blue')  

plt.plot(2, 0.75, 'o', markerfacecolor='white', markeredgecolor='blue')
plt.plot(2, 1, 'o', color='blue')  
plt.title("CDF for flipping a coin twice")
plt.show()
```

---

**Theorem**

Let $X$ have CDF $F$ and let $Y$ have CDF $G$. If $F(x)=G(X)$ for all $x$, then $\mathbb{P}(X\in A)=\mathbb{P}(Y\in A)$ for all $A$.

---

**Theorem**

A function $F$ mapping the real line to $[0, 1]$ is a CDF for some probability $\mathbb{P}$ if and only if $F$ satisfies:

1. $F$ is non-decreasing: $x_1 < x_2\Rightarrow F(x_{1})\leq F(x_{2})$.
1. $F$ is normalized 
$$
\lim_{x\rightarrow -\infty}F(x)=0,~\text{and} 
~\lim_{x\rightarrow\infty}F(x)=1.
$$
1. $F$ is right-continuous: $F(x) = F(x^{+})$ for all $x$, where 
$$
F(x^{+}) = \lim_{y\rightarrow x^{+}}F(y).
$$

---

$X$ is discrete if it takes countably many values $\{x_{1},x_{2},\ldots\}$. We define the *probability mass function* (pmf) for $X$ by $f_{X}(x) = \mathbb{P}(X=x)$.

Thus, $f_{X}(x)\geq 0$ for all $x\in\mathbb{R}$ and $\sum_{i}f_{X}(x_{i})=1$.

The CDF of $X$ is related to $f_{X}$ by

$$
F_{X}(x) = \mathbb{P}(X\leq x) = \sum_{x_{i}\leq x}f_{X}(x_{i}).
$$

---

What is the pmf for our example of flipping a coin twice?

---

A random variable $X$ is *continuous* if there exists a function $f_{X}$ such that $f_{X}(x)\geq 0$ for all $x$, $\int_{-\infty}^{\infty}f_{X}(x)dx = 1$ and for every $a\leq b$,

$$
\mathbb{P}(a < x < b) = \int_{a}^{b}f_{X}(x)dx.
$$

The function $f_{X}$ is called the *probability density function* (pdf). 

---

We have that
$$
F_{X}(x) = \int_{-\infty}^{x}f_{X}(t)dt,
$$
and $f_{X}(x) = F^{\prime}_{X}(x)$ at all points $x$ at which $F_{X}$ is differentiable.

---

Let $F$ be the CDF for a random variable $X$. Then:

1. $\mathbb{P}(X=x) = F(x) - F(x^{-1})$, where $F(x^{-1}) = \lim_{y\rightarrow x^{-1}}F(y)$.
1. $\mathbb{P}(x<X\leq y) = F(y) - F(x)$.
1. $\mathbb{P}(X>x) = 1 - F(x)$.
1. If $X$ is continuous then
$$
\begin{align}
F(b)-F(a) &= \mathbb{P}(a< X <b) = \mathbb{P}(a\leq X \leq b) \\
\mathbb{P}(a< X \leq b) = \mathbb{P}(a\leq X \leq b)
\end{align}
$$


## Some Important Discrete and Random Variables

### Point Mass Distribution

$X$ has a point mass distribution at $a$, written $X\sim \delta_{a}$, if $\mathbb{P}(X=a)=1$, in which case

$$
F(x) =
\begin{cases}
0 & x < a \\
1 & x\geq a.
\end{cases}
$$

The pmf is $f(x) = 1$ for $x=a$ and 0 otherwise.

---

### Discrete Uniform Distribution

Let $k>1$ be a given integer. Suppose that $X$ has pmf given by

$$
f(x) =
\begin{cases}
1/k & \text{for}~x=1,\ldots,k \\
0 & \text{otherwise},
\end{cases}
$$

then $X$ has a uniform distribution on $\{1,\ldots, k\}$.

---

### Bernoulli Distribution

Let $X$ represent a binary event with probability, $\mathbb{P}(X=1) = p$ and $\mathbb{P}(X=0)=1-p$ for some $p\in[0,1]$. We say that $X\sim \operatorname{Bernoulli}(p)$ has a Bernoulli distribution and 

$$
f(x)=
p^{x}(1-p)^{1-x},~\text{for}~x\in\{0, 1\}.
$$

---

### Binomial Distribution

Suppose we have a binary event with probability $p\in[0,1]$ and consider a sequence of $n$ independent events, the pmf is

$$
f(x) =
\begin{cases}
{n}\choose{x}p^{x}(1-p)^{n-x} & \text{for}~x=0,\ldots, n\\
0 & \text{otherwise}. 
\end{cases}
$$

A random variable with this pmf is a Binomial random variable $X\sim \operatorname{Binomial}(n, p)$. 

The variables $n$ and $p$ are fixed parameters, whereas $x$ is a particular value of the random variable.

---

### Geometric Distribution

A random variable $X$ has a geometric distribution $X\sim \operatorname{Geom}(p)$ with parameter $p\in(0,1)$ if

$$
\mathbb{P}(X=k) = p(1-p)^{k-1}, \quad k\geq 1.
$$

---

### Poisson Distribution

A random variable $X$ has a Poisson distribution $X\sim \operatorname{Poisson}(\lambda)$, with parameter $\lambda$ if

$$
f(x) = e^{-\lambda}\frac{\lambda^{x}}{x!} \quad x\geq 0.
$$

The Poisson distribution is often used as a model for counts of rare events like radioactive decay, traffic accidents, and horse kicks to the head.

## Some Important Continuous Random Variables

### Uniform Distribution

The random variable is uniformly distributed $X\sim \operatorname{Uniform}(a,b)$ on the interval $[a,b]$ ($a<b$) if

$$
f(x) =
\begin{cases}
\frac{1}{b-a} & \text{for}~x\in[a,b] \\
0 & \text{otherwise}.
\end{cases}
$$

---

The CDF is

$$
F(x) =
\begin{cases}
0 & x<a \\
\frac{x-a}{b-a} & x\in[a,b] \\
1 & x>b.
\end{cases}
$$

---

### Normal (Gaussian)

The random variable $X$ has a Normal (Gaussian)  distribution $X\sim N(\mu, \sigma^{2})$ with parameters $\mu\in\mathbb{R}$ and $\sigma>0$ if

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}}\operatorname{exp}\left\{-\frac{1}{2\sigma^{2}}(x-\mu)^{2} \right\},\quad x\in\mathbb{R}.
$$

The parameter $\mu$ is the mean of the distribution and $\sigma$ is the standard deviation.

$X$ has a standard normal distribution if $\mu=0$ and $\sigma=1$.

---

### Exponential Distribution

The random variable $X$ has an exponential distribution $X\sim \operatorname{Exp}(\beta)$ with parameter $\beta>0$ if

$$
f(x) = \frac{1}{\beta}e^{-x/\beta},\quad x>0.
$$

The exponential distribution is used to model the lifetime of electronic components and the waiting times between rare events.

---

### Gamma Distribution

The gamma function is defined by

$$
\Gamma(\alpha) = \int_{0}^{\infty}y^{\alpha-1}e^{-y}dy.
$$

The gamma distribution for a random variable $X\sim \operatorname{Gamma}(\alpha, \beta)$ with $\alpha,\beta >0$ is defined by

$$
f(x) = \frac{1}{\beta^{\alpha}\Gamma(\alpha)}x^{\alpha-1}e^{-x/\beta}.
$$

The exponential distribution is a $\operatorname{Gamma}(1,\beta)$ distribution.

---

### t Distribution

$X$ has a $t$ distribution with $\nu$ degrees of freedoom, $X\sim t_{\nu}$ if

$$
f(x) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)} \frac{1}{\left(1 + \frac{x^{2}}{\nu}\right)^{(\nu +1)/2}}.
$$

The $t$-distribution is similar to the Normal but has thicker tails at the end of the distribution.

---

### Cauchy Distribution

The Cauchy distribution is a $t$ distribution with $\nu=1$. The density is given by

$$
f(x) = \frac{1}{\pi(1+x^{2})}.
$$


## Bivariate Distributions

Given a pair of discrete random variables $X$ and $Y$, the joint mass function is defined as

$$
f(x, y) = \mathbb{P}(X=x, Y=y).
$$

We can be more specific and write $f_{X,Y}$.

---

In the continuous case, a pdf for the random variable $(X, Y)$ is a function $f(x, y)$ such that

1. $f(x, y) \geq 0$ for all $(x, y)$,
1. $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y) dxdy = 1$ and,
1. for any set $A\subset \mathbb{R}\times \mathbb{R}$, $\mathbb{P}((X,Y)\in A) = \int\int_{A}f(x,y)dxdy$.

In the discrete or continuous case the joint CDF is

$$
F_{X,Y}(x,y) = \mathbb{P}(X\leq x, Y\leq y).
$$

## Marginal Distributions

If $(X,Y)$ have joint distribution with mass function $f_{X,Y}$, then the *marginal mass function* for $X$ is defined by

$$
f_{X}(x) = \sum_{y}\mathbb{P}(X=x, Y=y) = \sum{y}f(x,y).
$$

and the *marginal mass function* for $Y$ is

$$
f_{Y}(y) = \sum_{x}\mathbb{P}(X=x, Y=y) = \sum{x}f(x,y).
$$

---

For continuous random variables, the marginal density functions are

$$
f_{X}(x) = \int f(x,y)dy, \quad\text{and}\quad f_{Y}(y) = \int f(x,y)dx.
$$

The corresponding marginal distribution functions are $F_{X}$ and $F_{y}$.

## Independent Random Variables

Two random variables $X$ and $Y$ are *independent* if for every $A$ and $B$,

$$
\mathbb{P}(X\in A, Y\in B) = \mathbb{P}(X\in A)\matbb{P}(Y\in B).
$$

If this condition is not satisfied then $X$ and $Y$ are *dependent*.

---

Let $X$ and $Y$ have joint PDF $f_{X,Y}$, then $X$ and $Y$ are independent if and only if 

$$
f_{X,Y}(x,y) = f_{X}(x)f_{Y}(y),
$$

for all values $x$ and $y$.

## Conditional Distributions

For discrete random variables, the *conditional probability mass function$ is

$$
\small
f_{X\vert Y}(x\vert y) = \mathbb{P}(X=x\vert Y=y) = \frac{\mathbb{P}(X=x, Y=y)}{\mathbb{P}(Y=y)} = \frac{f_{X,Y}(x,y)}{f_{Y}(y)},
$$

if $f_{Y}(y)>0$.

---

For continuous random variables, the *conditional probability density function* is

$$
f_{X\vert Y}(x\vert y) = \frac{f_{X,Y}(x,y)}{f_{Y}(y),}
$$

assuming that $f_{Y}(y)>0$. Then,

$$
 \mathbb{P}(X\in A\vert Y=y) = \int_{A}f_{X\vert Y}(x\vert y)dx.
$$

## Multivariate Distributions and IID Samples

Given $n$ random variables $X_{i}$, the *random vector* is defined as $X=(X_{1},\ldots, X_{n})$.

Let $f(x_{1},\ldots, x_{n})$ denote the multivariate PDF. The random variables are independent if

$$
f(x_{1},\ldots, x_{n}) = \prod_{i=1}^{n}f_{X_{i}}(x_{i}).
$$

---

If $X_{1},\ldots, X_{n}$ are independent and each has the same marginal distribution with CDF $F$, we say that $X_{1},\ldots X_{n}$ are independent and identically distributed (IID) and we write

$$
X_{1},\ldots X_{n}\sim F.
$$

If $F$ has density $f$ we also write $X_{1},\ldots X_{n}\sim f$. The variables $X_{1},\ldots X_{n}$ are called a *random sample* of size $n$ from $F$.

## The Multivariate Normal

A random vector $X = (X_{1},\ldots X_{n})$ has a multivariate normal distribution $X\sim N(\mu, \Sigma)$ if

$$
\small
f(\mathbf{x}; \mu, \Sigma) = \frac{1}{(2\pi)^{k/2}\vert \Sigma \vert^{1/2}}\operatorname{exp}\left\{-\frac{1}{2}(\mathbf{x}-\mu)^{T}\Sigma^{-1}(\mathbf{x}-\mu) \right\},
$$

where $\mu\in\mathbb{R}^{n}$ is the vector of means of each $X_{i}$, $\Sigma\in\mathbb{R}^{n\times n}$ is the symmetric positive definite covariance matrix, and $\vert \Sigma \vert$ is the determinant of $\Sigma$.

---

### Picture

## Transformations of Random Variables

Suppose that $X$ is a random variable with PDF $f_{X}$ and CDF $F_{X}$. Let $Y=r(X)$ be a function of $X$. 

The new variable $Y$ is a *transformation* of $X$. 

How do we compute the PDF and CDF of $Y$?

---

To find $f_{Y}$ follow these three steps:

1. For each $y$, find the set $A_{y}=\{x:~r(x)\leq y\}$.
1. Find the CDF

$$
\small
\begin{align}
F_{Y}(y) &= \mathbb{P}(Y\leq y) =  \mathbb{P}(r(X)\leq y) \\
&= \mathbb{P}(\{x:~r(x)\leq y\}) \\
&= \int_{A_{y}}f_{X}(x)dx.
\end{align} 
$$

1. The PDF is $f_{Y}(y) = F^{\prime}_{Y}(y)$.

The steps for functions of several random variables are similar.

---

### Example

## Summary

We covered:

- Random variables
- CDFs
- PMFs and important discrete distributions
- PDFs and important continuous distributions
- Bivariate (joint) and marginal distributions
- Conditional distributions
- the Multivariate Normal distribution
- Transformations of random variables