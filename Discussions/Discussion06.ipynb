{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e771a7",
   "metadata": {},
   "source": [
    "# Discussion 6\n",
    "\n",
    "In this discussion we review integration and optimization by focusing on their computational aspects. \n",
    "\n",
    "You can use the Shared Computing Cluster (SCC) or Google Colab to run this notebook.\n",
    "\n",
    "The general instructions for running on the SCC are available under General Resources on [Piazza](https://piazza.com/bu/fall2025/ds722/resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddfb2c4",
   "metadata": {},
   "source": [
    "## Problem 1: Numerical Integration\n",
    "\n",
    "In this problem we investigate two methods for numerical integration:\n",
    "\n",
    "- the trapezoid rule  \n",
    "- Simpson's rule\n",
    "\n",
    "---\n",
    "\n",
    "## Trapezoid Rule\n",
    "\n",
    "The Trapezoid Rule approximates the area under the curve using trapezoids:\n",
    "\n",
    "$$\n",
    "\\int_a^b f(x)\\,dx \\approx \\frac{h}{2} \\left[ f(x_0) + 2 \\sum_{i=1}^{n-1} f(x_i) + f(x_n) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ h = \\frac{b - a}{n} $\n",
    "- $ x_i = a + i h $ for $ i = 0, 1, \\dots, n $\n",
    "\n",
    "---\n",
    "\n",
    "## Simpson's Rule\n",
    "\n",
    "Simpsonâ€™s Rule approximates the integral by fitting parabolas through each pair of subintervals:\n",
    "\n",
    "$$\n",
    "\\int_a^b f(x)\\,dx \\approx \\frac{h}{3} \\left[ f(x_0) + 4 \\sum_{\\substack{i=1 \\\\ i \\text{ odd}}}^{n-1} f(x_i) + 2 \\sum_{\\substack{i=2 \\\\ i \\text{ even}}}^{n-2} f(x_i) + f(x_n) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ h = \\frac{b - a}{n} $, and $ n $ must be even\n",
    "- $ x_i = a + i h $ for $ i = 0, 1, \\dots, n $\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Implement these rules as Python functions.  \n",
    "2. Use your Python functions to approximate the integrals of:\n",
    "   - $ f_1(x) = \\sin{x} $ on $ [0, \\pi] $  \n",
    "   - $ f_2(x) = x^5 $ on $ [0, 1] $  \n",
    "3. Compute the error between your approximations and the true value of the integral as you double the number of points in your approximation.  \n",
    "4. Plot the errors on a log-log plot. What does the slope of the line tell you about the error?\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73494082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037dbe9",
   "metadata": {},
   "source": [
    "3. The slope of the line is the order of convergence. For every doubling of points, the error goes down by a factor of 2 in the trapezoid rule. For Simpson's rule, when you double the number of points, the error decreases by a factor of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee1139",
   "metadata": {},
   "source": [
    "## Problem 2: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df41efd",
   "metadata": {},
   "source": [
    "Consider the objective function\n",
    "\n",
    "$$\n",
    "f(x)=\\ln(e^{2x}+e^{-2x})\n",
    "$$\n",
    "\n",
    "This objective function is plotted in the following cell. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc924ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function f(x) = ln(e^(2x) + e^(-2x))\n",
    "def f(x):\n",
    "    return np.log(np.exp(2*x) + np.exp(-2*x))\n",
    "\n",
    "# Generate x values in the range [-2, 2]\n",
    "x = np.linspace(-2, 2, 400)\n",
    "y = f(x)\n",
    "\n",
    "# Plot the function\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Plot of $f(x) = \\\\ln(e^{2x} + e^{-2x})$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5406f",
   "metadata": {},
   "source": [
    "\n",
    "For this problem we wish to investigate the performance of the following optimization methods:\n",
    "\n",
    "- Gradient descent: $x_{k+1} = x_{k} - \\alpha_{k}\\nabla f_{k}$\n",
    "- Newton's method: $x_{k+1} = x_{k} - \\alpha_{k}\\nabla^{2}f_{k}^{-1} \\nabla f_{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b461c47",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Compute the gradient of this function by hand and determine the minimum solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30245a9",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Compute the Hessian of this function by hand and confirm it is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435faea",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Write Python functions for the objective function $f(x)$, the gradient function $\\nabla f(x)$, and the Hessian $\\nabla^{2} f(x)$. You will need to compute the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec785423",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Write a function `gradient_descent(x0, alpha, k)` which performs $k$ steps of the gradient descent method with learning rate $\\alpha$ and initial step $x_{0}$. The function should return the final iterate $x_{k}$ and the objective function value $f(x_{k})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63241878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f126459",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Write a function `newton_method(x0, alpha, k)` which performs $k$ steps of Newton's method with learning rate $\\alpha$ and initial step $x_{0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131730fc",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "Let $x_{0}=0.5$. Run 10 steps of gradient descent and Newton's method with $\\alpha_{k}=0.1$. Display a table of the values $x_{k}$ and the errors, i.e, $\\vert f(x_{k}) - f(x^{*})\\vert$ for each $k=1,\\ldots, 10$ and for both methods. Which method converges to the true solution faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1caeff5",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "\n",
    "Let $x_{0}=1.2$. Run 10 steps of gradient descent and Newton's method with $\\alpha_{k}=0.1$. Display a table of the values $x_{k}$ and the errors, i.e, $\\vert f(x_{k}) - f(x^{*})\\vert$ for each $k=1,\\ldots, 10$ and for both methods. Do both methods converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds722",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
