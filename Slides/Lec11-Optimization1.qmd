---
title: "Optimization"
jupyter: python3
---

## Lecture Overview

Optimization is widely used in science, engineering, economics, and industry. As a data scientist it is essential to develop an understanding of optimization algorithms.

Examples of optimization in data science:

:::: {.incremental}
- Model training (loss function minimization)
- Clustering (minimizing the WCSS)
- Portfolio optimization (maximize return while minimizing risk)
- Resource allocation (e.g., computational resources, manufacturing, supply chains)
::::

## Lecture Objectives

Understand:

:::: {.incremental}
- Mathematical formulation of optimization problems
- Unconstrained vs. constrained optimization
- First order methods (gradient descent and variants)
::::

## Optimization

The most generic way to formulate a (unconstrained) minimization problem is

$$
\min_{x} f(x),
$$

where $x\in\mathbb{R}^{n}$ is a real vector with $n\geq 1$ components and $f: \mathbb{R}^{n}\rightarrow \mathbb{R}$.

The function $f$ is referred to as an objective function.

---

Suppose we have data measurements $\left\{(t_i, y_i)\right\}_{i=1}^{m}$ and we are looking to find coefficients $x = [x_1, x_2]$ such that 

$$
\small
\phi(t; x)=x_{2}t + x_{1}.
$$ 

Define $r_{i}(x) = y_i - \phi(t_i;x)$ then 

$$
\small
\min_{x} \sum_{i=1}^{m}r_{i}^{2}(x) = \Vert y - Ax\Vert^{2},
$$
where $\small A = \begin{bmatrix} 1 & t_1 \\ 1 & t_2 \\ \vdots & \vdots \\ 1 & t_m \end{bmatrix}$ and $x = \begin{bmatrix}x_{1} \\ x_{2}\end{bmatrix}$.

## Constrained Optimization

A constrained optimization problem is
$$
\min_{x} f(x) \qquad\text{subject to}\quad 
\begin{array}{l}
c_i(x) = 0 & i\in\mathcal{E}, \\
c_{i}(x) \geq 0 & i\in\mathcal{I},
\end{array}
$$

where $c_{i}$ are constraint functions and $\mathcal{I},\mathcal{E}$ are sets of indices.

---

Consider the following example of a constrained optimization problem:

$$
\small
\min (x_1 - 2)^{2} + (x_2 -1)^{2} \qquad\text{s.t}\quad 
\begin{array}{l}
-x_{1}^{2} + x_2 &\geq 0 \\
-x_1 - x_2 + 2 &\geq 0. 
\end{array}
$$

---

```{python}
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Define the function
def f(x1, x2):
    return (x1 - 2)**2 + (x2 - 1)**2

# Define the constraints
def constraint1(x1):
    return x1**2

def constraint2(x1):
    return 2 - x1

# Create a grid of points
x1 = np.linspace(0, 2, 400)
x2 = np.linspace(0, 2, 400)
X1, X2 = np.meshgrid(x1, x2)
Z = f(X1, X2)

# Plot the contours of the function
plt.contour(X1, X2, Z, levels=20, cmap='viridis')

# Plot the feasible region
x1_feasible = np.linspace(0, np.sqrt(2), 400)
x2_feasible1 = constraint1(x1_feasible)
x2_feasible2 = constraint2(x1_feasible)

plt.fill_between(x1_feasible, x2_feasible1, x2_feasible2, where=(x2_feasible2 >= x2_feasible1), color='gray', alpha=0.5)

# Find the minimum point within the feasible region
from scipy.optimize import minimize

def objective(x):
    return f(x[0], x[1])

constraints = [
    {'type': 'ineq', 'fun': lambda x: x[1] - x[0]**2},
    {'type': 'ineq', 'fun': lambda x: 2 - x[0] - x[1]}
]

result = minimize(objective, [0, 0], constraints=constraints)
x_min, y_min = result.x

# Plot the minimum point
plt.plot(x_min, y_min, 'r*', markersize=15)
plt.text(x_min, y_min, '  Minimum', verticalalignment='bottom', horizontalalignment='right')

## Plot feasible lines
plt.plot(x1, 2-x2, color='gray')
plt.plot(x1_feasible, x1_feasible**2, color='gray')


# Set labels and title
plt.xlabel('$x_1$')
plt.ylabel('$x_2$')
plt.title('Contour plot with feasible region and minimum point')
plt.grid(True)
plt.show()
```

## Optimization Algorithms

Optimization algorithms are iterative. They start from an initial guess of the variable $x$ and generate a sequence of improved estimates that converge to a solution.

The strategy of how to move from one iterate to the next distinguishes one algorithm from another.

Most strategies use:

- values of the objective function
- the constraint function
- possibly first and/or second derivative information
- information from previous iterations

---

Each optimization algorithm should possess the following properties

:::: {.incremental}
- Robustness. Perform well on a wide variety of problems in their class.
- Efficiency. Not require excessive computing time or memory.
- Accuracy. Identify a solution with precision, without being overly sensitive to errors in the data.
::::


## What is a solution?

A point $x^{*}$ is a global minimizer if $f(x^{*}) \leq f(x)$ for all $x$.

```{python}
#| label: fig-global-min
#| fig-cap: "Global Minimum"
#| fig-align: "center"
import matplotlib.pyplot as plt
import numpy as np

# Define the function y = x^2
def f(x):
    return x**2

# Generate x values
x = np.linspace(-10, 10, 400)
y = f(x)

# Plot the function
plt.plot(x, y, label='$y = x^2$')

# Mark the global minimum with a red star
global_min_x = 0
global_min_y = f(global_min_x)
plt.plot(global_min_x, global_min_y, 'r*', markersize=15, label='global minimizer')

# Add labels and title
plt.xlabel('x')
plt.ylabel('y')
plt.title('Plot of $y = x^2$ with global minimizer')
plt.legend()

# Show the plot
plt.grid(True)
plt.show()
```

---

A point $x^{*}$ is a local minimizer if there is a neighborhood $\mathcal{N}$ of $x^{*}$ such that $f(x^{*}) \leq f(x)$ for all $x\in\mathcal{N}$.

```{python}
#| label: fig-local-min
#| fig-cap: "Local Minima"
#| fig-align: "center"
import matplotlib.pyplot as plt
import numpy as np

# Define the function y = 2 + cos(x) + cos(2x-0.5)/2
def f(x):
    return 2 + np.cos(x) + np.cos(2*x-0.5)/2

# Generate x values
x = np.linspace(0, 2*np.pi, 400)
y = f(x)

# Plot the function
plt.plot(x, y, label=r'$y = 2 + \cos{x} + \frac{\cos{2x-0.5}}{2}$')

# Mark the global minimum with a red star
global_min_x = 2.2611
local_min_x = 4.3555
global_min_y = f(global_min_x)
local_min_y = f(local_min_x)
plt.plot(global_min_x, global_min_y, 'r*', markersize=15, label='global minimizer')
plt.plot(local_min_x, local_min_y, 'ro', markersize=10, label='local minimizer')

# Add labels and title
plt.xlabel('x')
plt.ylabel('y')
plt.title(r'Plot of $y = 2 + \cos{x} + \frac{\cos{2x-0.5}}{2}$ with global and local minimizers')
plt.legend()

# Show the plot
plt.grid(True)
plt.show()
```


## Convexity

The concept of convexity is fundamental in optimization. 

In a convex optimization problem, any local minimum is also a global minimum.

Convex functions have smooth and predictable curvature meaning:

- no valleys or humps
- well-behaved gradients

---

A set $S\subset \mathbb{R}^{n}$ is convex if for any two points $x,y\in S$ then $\alpha x + (1-\alpha)y\in S$ for all $\alpha\in[0, 1]$.

This means that the straight line segment connecting any 2 points in the set lies entirely inside the set.


## Example: Convex Set

```{python}
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt

# Create points for the unit circle
theta = np.linspace(0, 2*np.pi, 400)
x = np.cos(theta)
y = np.sin(theta)

# Plot the unit circle
plt.figure(figsize=(6,6))
plt.plot(x, y, label='Unit Circle')

# Fill the interior to show convexity
plt.fill(x, y, color='lightblue', alpha=0.5, label='Convex Set')

# Pick two points inside the circle
A = np.array([0.5, 0.5])
B = np.array([-0.7, 0.6])
plt.plot([A[0], B[0]], [A[1], B[1]], 'ro', label='Points A & B')

# Draw the line segment between A and B
plt.plot([A[0], B[0]], [A[1], B[1]], 'r-', linewidth=2, label='Line Segment AB')

plt.xlabel('$x_1$')
plt.ylabel('$x_2$')
plt.title('Unit Circle as a Convex Set')
plt.axis('equal')
plt.legend()
plt.grid(True)
plt.show()
```

---

Let $S = \{x \in \mathbb{R}^2 : \Vert x \Vert_2 \leq 1\}$ denote the unit circle (including its interior).

**Proof**

::: {.notes}
To show $S$ is convex, take any $x, y \in S$ and any $\alpha \in [0, 1]$. Consider $z = \alpha x + (1-\alpha) y$.

By the triangle inequality for the Euclidean norm:
$$
\Vert z\Vert_2 = \Vert\alpha x + (1-\alpha) y\Vert_2 \leq \alpha \Vert x\Vert_2 + (1-\alpha) \Vert y\Vert_2 \leq \alpha \cdot 1 + (1-\alpha) \cdot 1 = 1
$$

Thus, $z \in S$. Therefore, the unit circle is convex.
:::


---

The function $f$ is a convex function if its domain $S$ is a convex set and if for any two points $x,y\in S$ the following property is satisfied

$$
\small
f(\alpha x + (1-\alpha)y) \leq \alpha f(x) + (1-\alpha)f(y),\quad \text{for all}~\alpha\in[0,1].
$$

For a convex function the line connecting the points $(x, f(x))$ $(y, f(y))$ must lie above the graph of $f$.

## Example: Convex Function

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Define a convex function, e.g., f(x) = 1/x
def f(x):
    return 1/x

x = np.linspace(0.01, 10, 400)  # 1/x is only defined for x > 0
y = f(x)

plt.plot(x, y, label='$f(x) = 1/x$')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Example of a Convex Function: $f(x) = 1/x$')
plt.legend()
plt.grid(True)
plt.show()

```

---

**Proof**

::: {.notes}
Let $f(x) = 1/x$ for $x > 0$. To show $f$ is convex, we check the definition:

For any $x, y > 0$ and $\alpha \in [0, 1]$,
$$
f(\alpha x + (1-\alpha)y) = \frac{1}{\alpha x + (1-\alpha)y}
$$
and
$$
\alpha f(x) + (1-\alpha)f(y) = \frac{\alpha}{x} + \frac{1-\alpha}{y}
$$

We need to show:
$$
\frac{1}{\alpha x + (1-\alpha)y} \leq \frac{\alpha}{x} + \frac{1-\alpha}{y}
$$


This is true by the weighted arithmetic-harmonic mean inequality.

:::

---

The second derivative test is another way to determine if a function is convex.

If $f^{\prime\prime}(x)>0$, then $f(x)$ is convex (concave up).

When $x$ is a vector, the equivalent condition is that $\nabla^{2}f(x)$ is positive definite.

---

What is the 2nd derivative of $f(x)=\frac{1}{x}$?


## Recognizing a Local Minimum

How do we recognize a local minimum $x^{*}$?

:::: {.fragment}
When a function is *smooth* (differentiable) there are efficient and practical ways to identify a local minima.

We will use information from the gradient $\nabla f(x^{*})$ and the Hessian $\nabla^{2} f(x^{*})$.
::::

---

The mathematical tool that we will use to study minimizers of smooth functions is Taylor's theorem.

**Theorem**

Suppose that $f:\mathbb{R}^{n}\rightarrow \mathbb{R}$ is twice continuously differentiable and that $p\in\mathbb{R}^{n}$. Then we have that

$$
f(x+tp) = f(x) + t\nabla f(x+tp)^{T}p + \frac{t^2}{2}p^{T}\nabla^{2} f(x+tp)p,
$$
for some $t\in(0,1)$.

## First Order Necessary Conditions

Necessary conditions are derived by assuming that $x^{*}$ is a local minimizer and then proving facts about $\nabla f(x^{*})$ and $\nabla^{2} f(x^{*})$.

**Theorem**
If $x^{*}$ is a local minimizer and $f$ is continuously differentiable in an open neighborhood of $x^{*}$, then $\nabla f(x^{*})=0$.


## Second Order Necessary Conditions

**Theorem**
If $x^{*}$ is a local minimizer and $f$ and $\nabla^{2} f$ exists and is continuously differentiable in an open neighborhood of $x^{*}$, then $\nabla f(x^{*})=0$ and $\nabla^{2} f(x^{*})$ is positive semidefinite, i.e., $p^{T}\nabla^{2} f(x^{*})p \geq 0~\forall p$.



## Second Order Sufficient Conditions

Sufficient conditions are conditions on the derivatives of $f$ that guarantee that $x^{*}$ is a local minimizer.

**Theorem**
Suppose that $\nabla^{2} f$ is continuous in an open neighborhood of $x^{*}$ and that $\nabla f(x^{*})=0$ and $\nabla^{2} f(x^{*})$ is positive definite, i.e., $p^{T}\nabla^{2} f(x^{*})p > 0~\forall p$. Then $x^{*}$ is a local minimizer of $f$.

## From Local to Global

**Theorem**
When $f$ is convex, any local minimizer $x^{*}$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $x^{*}$ such that $\nabla f(x^{*})=0$ is a global minimizer of $f$.

## Overview of Algorithms

There are two fundamental strategies for moving from a current point $x_{k}$ to a new iterate $x_{k+1}$:

- line search
- trust region

We will focus on line search algorithms.

## Line Search Methods

Line search algorithms choose a direction $p_{k}$ and searches along this direction from the current iterate $x_{k}$ for a new iterate with a lower objective function value, i.e., compute

$$
x_{k+1} = x_{k} + \alpha p_{k},
$$

such that $f(x_{k+1}) < f(x_{k})$.

---

Each iteration of a line search method computes a search direction $p_{k}$ and then creates the new iterate

$$
x_{k+1} = x_{k} + \alpha_{k}p_{k}.
$$

The value $\alpha_{k}$ is called the step length.

The success of a line search method depends on both the direction and the step length.

---

The search direction often has the form

$$
p_{k} = -B_{k}^{-1}\nabla f_{k},
$$

where $B_{k}$ is a symmetric and nonsingular matrix and $\nabla f_{k}=\nabla f(x_{k})$.

Different choices for $B_{k}$ lead to different methods.

## Trust Region Methods

Trust region algorithms use information about $f$ to construct a model function $m_{k}$ such that $m_{k}(x_{k}) \approx f(x_{k})$.

The model $m_{k}$ is generally a poor approximation of $f$ for values of $x$ that are far away from $x_{k}$.

Look for a candidate step $p$ that is within a small region around $x_{k}$ satisfying

$$
\min_{p} m_{k}(x_{k}+p).
$$

If $f(x_{k}+p)$ is not smaller than $f(x_{k})$, then we reduce the trust region and try again.

## Gradient Descent

Gradient descent is a line search method where $B_{k}=I$ so that $p_{k}=-\nabla f_{k}$. The iterative update is the familiar

$$
x_{k+1} = x_{k} -\alpha_{k}\nabla f_{k}.
$$

Gradient descent is called a first order method because it relies solely on first-order derivatives in the gradient to update $x_{k+1}$.

## Newton's Method

Newton's method is a line search method where $B_{k}=-\nabla^{2} f_{k}$ so that $p_{k}=-(\nabla^{2} f_{k})^{-1}\nabla f_{k}$. The iterative update is
$$
x_{k+1} = x_{k} -\alpha_{k}(\nabla^{2} f_{k})^{-1}\nabla f_{k}.
$$

Newton's method is called a second order method because it relies on second-order derivatives in the Hessian matrix to update $x_{k+1}$.

## Quasi-Newton Methods

Quasi-Newton methods are line search methods where $B_{k}\approx-\nabla^{2} f_{k}$. The search direction is then $p_{k}=-B_{k}^{-1}\nabla f_{k}$. The iterative update is

$$
x_{k+1} = x_{k} -\alpha_{k}B_{k}^{-1}\nabla f_{k}.
$$

Quasi-Newton methods attempt to avoid the construction of the full Hessian matrix while maintaining the desirable convergence properties of Newton methods. 

The BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm is one such method.

## Rates of Convergence

Let $\{x_{k}\}$ be a sequence in $\mathbb{R}^{n}$ that converges to $x^{*}$. 

### Linear

We say that convergence is *Q-linear* if there is a constant $r\in(0,1)$ such that

$$
\frac{\Vert x_{k+1}-x^{*}\Vert}{\Vert x_{k}-x^{*}\Vert}\leq r
$$

for all $k$ sufficiently large.

---

**Example**

Consider the sequence $x_k = \left(\frac{1}{2}\right)^k$ converging to $x^* = 0$.

Let's visualize its convergence

```{python}
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt

# Sequence definition
k = np.arange(0, 20)
x_k = (1/2)**k

fig, axs = plt.subplots(1, 2, figsize=(12,4))

# Standard plot
axs[0].plot(k, x_k, 'o-', label=r'$x_k = (1/2)^k$')
axs[0].axhline(0, color='gray', linestyle='--', label=r'$x^* = 0$')
axs[0].set_xlabel('Iteration $k$')
axs[0].set_ylabel(r'$x_k$')
axs[0].set_title('Linear Convergence (Standard Plot)')
axs[0].legend()
axs[0].grid(True)

# Semilog plot
axs[1].semilogy(k, x_k, 'o-', label=r'$x_k = (1/2)^k$')
axs[1].axhline(0, color='gray', linestyle='--', label=r'$x^* = 0$')
axs[1].set_xlabel('Iteration $k$')
axs[1].set_ylabel(r'$x_k$')
axs[1].set_title('Linear Convergence (Semilog Plot)')
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()
```


---

### Superlinear

The convergence is Q-superlinear if 

$$
\lim_{k\rightarrow \infty} \frac{\Vert x_{k+1}-x^{*}\Vert}{\Vert x_{k}-x^{*}\Vert}= 0.
$$

If a sequence converges *superlinearly* then it must converge linearly.

---

Consider the sequence $x_k = \frac{1}{k^{k}}$ converging to $x^* = 0$.

Let's visualize its superlinear convergence:

```{python}
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt

# Sequence definition
k = np.arange(1, 20)
x_k = 1 / k**k

fig, axs = plt.subplots(1, 2, figsize=(12,4))

# Standard plot
axs[0].plot(k, x_k, 'o-', label=r'$x_k = 1/k^k$')
axs[0].axhline(0, color='gray', linestyle='--', label=r'$x^* = 0$')
axs[0].set_xlabel('Iteration $k$')
axs[0].set_ylabel(r'$x_k$')
axs[0].set_title('Superlinear Convergence (Standard Plot)')
axs[0].legend()
axs[0].grid(True)

# Semilog plot
axs[1].semilogy(k, x_k, 'o-', label=r'$x_k = 1/k^k$')
axs[1].axhline(0, color='gray', linestyle='--', label=r'$x^* = 0$')
axs[1].set_xlabel('Iteration $k$')
axs[1].set_ylabel(r'$x_k$')
axs[1].set_title('Superlinear Convergence (Semilog Plot)')
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()
```

---

### Quadratic

We say that convergence is *Q-quadratic* if there is a positive constant $M$ such that

$$
\frac{\Vert x_{k+1}-x^{*}\Vert}{\Vert x_{k}-x^{*}\Vert^{2}}\leq M
$$

for all $k$ sufficiently large.

---

**Example**

Consider the sequence $x_k = \frac{1}{2^{2^{k}}}$ converging to $x^* = 0$.

Let's visualize its quadratic convergence:

```{python}
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt

# Sequence definition
k = np.arange(0, 20)
x_k = 1 / 2**(2**k)

fig, axs = plt.subplots(1, 2, figsize=(12,4))

# Standard plot
axs[0].plot(k, x_k, 'o-', label=r'$x_k = 1/2^{2^k}$')
axs[0].axhline(0, color='gray', linestyle='--', label=r'$x^* = 0$')
axs[0].set_xlabel('Iteration $k$')
axs[0].set_ylabel(r'$x_k$')
axs[0].set_title('Quadratic Convergence (Standard Plot)')
axs[0].legend()
axs[0].grid(True)

# Semilog plot
axs[1].semilogy(k, x_k, 'o-', label=r'$x_k = 1/2^{2^k}$')
axs[1].axhline(0, color='gray', linestyle='--', label=r'$x^* = 0$')
axs[1].set_xlabel('Iteration $k$')
axs[1].set_ylabel(r'$x_k$')
axs[1].set_title('Quadratic Convergence (Semilog Plot)')
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()
```

---

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt

# Define sequences
k_linear = np.arange(0, 20)
xk_linear = (1/2)**k_linear

k_superlinear = np.arange(1, 15)
xk_superlinear = 1 / k_superlinear**k_superlinear

k_quadratic = np.arange(0, 11)
xk_quadratic = 1 / 2**(2**k_quadratic)

fig, axs = plt.subplots(1, 2, figsize=(14,5))

# Standard plot: overlay all sequences
axs[0].plot(k_linear, xk_linear, 'o-', label='Linear: $(1/2)^k$')
axs[0].plot(k_superlinear, xk_superlinear, 's-', label='Superlinear: $1/k^k$')
axs[0].plot(k_quadratic, xk_quadratic, '^-', label='Quadratic: $1/2^{2k}$')
axs[0].axhline(0, color='gray', linestyle='--')
axs[0].set_xlabel('Iteration $k$')
axs[0].set_ylabel('$x_k$')
axs[0].set_title('Convergence Comparison (Standard Plot)')
axs[0].legend()
axs[0].grid(True)

# Semilog plot: overlay all sequences
axs[1].semilogy(k_linear, xk_linear, 'o-', label='Linear: $(1/2)^k$')
axs[1].semilogy(k_superlinear, xk_superlinear, 's-', label='Superlinear: $1/k^k$')
axs[1].semilogy(k_quadratic, xk_quadratic, '^-', label='Quadratic: $1/2^{2k}$')
axs[1].axhline(0, color='gray', linestyle='--')
axs[1].set_xlabel('Iteration $k$')
axs[1].set_ylabel('$x_k$')
axs[1].set_title('Convergence Comparison (Semilog Plot)')
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()
```

## Summary

We provided an overview of optimization. We covered

- mathematical formulation
- constrained vs. unconstrained optimization
- global and local minimizers
- iterative methods for solving optimization methods
- necessary and sufficient conditions for minimizers
- rates of convergence