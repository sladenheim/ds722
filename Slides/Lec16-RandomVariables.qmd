---
title: "Random Variables and Distributions"
jupyter: python3
---

## Lecture Overview

Statistics and data science are concerned with data. 

The link between sample spaces and events to data is through the concept of a *random variable*.

Random variables will allow us to formulate distributions.

## Lecture Objectives

Understand:

- definition and examples of random variables
- probability mass and density functions (pmf, pdf)
- important distributions

## Random Variable

Let $\Omega$ be a sample space. A *random variable* is a mapping 

$$
X: \Omega \rightarrow \mathbb{R},
$$

that assigns a real number $X(\omega)$ to each outcome $\omega$.

---

Flip a coin ten times. Let $X(\omega)$ be the number of heads in the sequence $\omega$. If 

$$
\omega = HTHTHTHTHT,
$$
then $X(\omega)=5$.

---

Let $\Omega = \{(x, y): x^{2} + y^{2} \leq 1\}$ be the unit disk. Assume we have a way to randomly sample this space, i.e., $\omega = (x, y)$. Then the following are random variables:

- $X(\omega) = x$
- $Y(\omega) = y$
- $Z(\omega) = x+y$
- $W(\omega) = \sqrt{x^{2}+y^{2}}$

---

Given a random variable $X$ and a subset $A$ of the real line, we define $X^{-1}(A) = \{\omega\in\Omega : X(\omega)\in A\}$ and let

$$
\begin{align}
\mathbb{P}(X\in A) &= \mathbb{P}(X^{-1}(A)) = \mathbb{P}(\{\omega\in\Omega : X(\omega)\in A\}) \\
\mathbb{P}(X = x) &= \mathbb{P}(X^{-1}(x)) = \mathbb{P}(\{\omega\in\Omega : X(\omega)=x\}) \\
\end{align}
$$

---

Flip a coin twice and let $X$ be the number of heads. Let's fill in the following two tables together

:::: {.columns}
::: {.column width="60%"}
| $\omega$ | $\mathbb{P}(\omega)$ | $X(\omega)$ |
|----------|----------------------|-------------|
:::
::: {.column width="40%"}
| $x$ | $\mathbb{P}(X=x)$ | 
|----------|--------------|
:::
::::

::: {.notes}
TT, TH, HT, HH
.25, .25, .25, .25
0, 1, 1, 2

0, 1, 2
.25, .5, .25
:::


## Distribution and Probability Functions

We introduce the following notions:

- cumulative distribution functions
- probability functions
    - probability mass functions (discrete)
    - probability density functions (continuous)

---

### Cumulative Distribution Functions

Given a random variable $X$, the *cumulative distribution function* (CDF) is the function

$$
F_{X}: \mathbb{R}\rightarrow [0, 1] 
$$
defined by

$$
F_{X}(x) = \mathbb{P}(X\leq x).
$$

The CDF contains all the information about the random variable.

---

Flip a fair coin twice and let $X$ be the number of heads. Then $\mathbb{P}(X=0) = \mathbb{P}(X=2) = 1/4$ and $\mathbb{P}(X=1) = 1/2$, the CDF is

$$
F_{X}(x) = 
\begin{cases}
0 & x < 0 \\
1/4 & 0\leq x < 1 \\
3/4 & 1\leq x < 2 \\
1 & x\geq 2.
\end{cases}
$$

---

```{python}
import matplotlib.pyplot as plt
import numpy as np

x0 = np.linspace(-1, 0, 50)
y0 = 0*x0

x1 = np.linspace(0, 1, 50 )
y1 = 0.25*np.ones(len(x1))

x2 = np.linspace(1, 2, 50)
y2 = 0.75*np.ones(len(x1))

x3 = np.linspace(2, 3, 50)
y3 = np.ones(len(x3))

plt.plot(x0, y0, color='blue')
plt.plot(x1, y1, color='blue')
plt.plot(x2, y2, color='blue')
plt.plot(x3, y3, color='blue')

plt.plot(0, 0, 'o', markerfacecolor='white', markeredgecolor='blue')
plt.plot(0, 0.25, 'o', color='blue')  

plt.plot(1, 0.25, 'o', markerfacecolor='white', markeredgecolor='blue')
plt.plot(1, 0.75, 'o', color='blue')  

plt.plot(2, 0.75, 'o', markerfacecolor='white', markeredgecolor='blue')
plt.plot(2, 1, 'o', color='blue')  
plt.title("CDF for flipping a coin twice")
plt.show()
```

---

**Theorem**

Let $X$ have CDF $F$ and let $Y$ have CDF $G$. If $F(x)=G(x)$ for all $x$, then $\mathbb{P}(X\in A)=\mathbb{P}(Y\in A)$ for all $A$.

---

**Theorem**

A function $F$ mapping the real line to $[0, 1]$ is a CDF for some probability $\mathbb{P}$ if and only if $F$ satisfies:

1. $F$ is non-decreasing: $x_1 < x_2\Rightarrow F(x_{1})\leq F(x_{2})$.
1. $F$ is normalized 
$$
\small
\lim_{x\rightarrow -\infty}F(x)=0,~\text{and} 
~\lim_{x\rightarrow\infty}F(x)=1.
$$
1. $F$ is right-continuous: $F(x) = F(x^{+})$ for all $x$, where 
$$
\small
F(x^{+}) = \lim_{y\rightarrow x^{+}}F(y).
$$

---

$X$ is discrete if it takes countably many values $\{x_{1},x_{2},\ldots\}$. We define the *probability mass function* (pmf) for $X$ by $f_{X}(x) = \mathbb{P}(X=x)$.

Thus, $f_{X}(x)\geq 0$ for all $x\in\mathbb{R}$ and $\sum_{i}f_{X}(x_{i})=1$.

The CDF of $X$ is related to $f_{X}$ by

$$
F_{X}(x) = \mathbb{P}(X\leq x) = \sum_{x_{i}\leq x}f_{X}(x_{i}).
$$

---

What is the pmf for our example of flipping a coin twice?

::: {.notes}
$$
f_{X}(x) = 
\begin{cases}
1/4 & x=0 \\
1/2 & x=1 \\
1/4 & x=2 \\
0 & \text{otherwise}
\end{cases}
$$
:::

---

A random variable $X$ is *continuous* if there exists a function $f_{X}$ such that $f_{X}(x)\geq 0$ for all $x$, $\int_{-\infty}^{\infty}f_{X}(x)dx = 1$ and for every $a\leq b$,

$$
\mathbb{P}(a < x < b) = \int_{a}^{b}f_{X}(x)dx.
$$

The function $f_{X}$ is called the *probability density function* (pdf). 

---

We have that
$$
F_{X}(x) = \int_{-\infty}^{x}f_{X}(t)dt,
$$
and $f_{X}(x) = F^{\prime}_{X}(x)$ at all points $x$ at which $F_{X}$ is differentiable.

---

Let $F$ be the CDF for a random variable $X$. Then:

1. $\mathbb{P}(X=x) = F(x) - F(x^{-})$, \ 
where $F(x^{-}) = \lim_{y\rightarrow x^{-}}F(y)$.
1. $\mathbb{P}(x<X\leq y) = F(y) - F(x)$.
1. $\mathbb{P}(X>x) = 1 - F(x)$.
1. If $X$ is continuous then
$$
\begin{align}
F(b)-F(a) &= \mathbb{P}(a< X <b) = \mathbb{P}(a\leq X \leq b) \\
&= \mathbb{P}(a< X \leq b) = \mathbb{P}(a\leq X \leq b).
\end{align}
$$


## Some Important Discrete and Random Variables

### Point Mass Distribution

$X$ has a point mass distribution at $a$, written $X\sim \delta_{a}$, if $\mathbb{P}(X=a)=1$, in which case

$$
F(x) =
\begin{cases}
0 & x < a \\
1 & x\geq a.
\end{cases}
$$

The pmf is $f(x) = 1$ for $x=a$ and 0 otherwise.

---

### Discrete Uniform Distribution

Let $k>1$ be a given integer. Suppose that $X$ has pmf given by

$$
f(x) =
\begin{cases}
1/k & \text{for}~x=1,\ldots,k \\
0 & \text{otherwise},
\end{cases}
$$

then $X$ has a uniform distribution on $\{1,\ldots, k\}$.

---

### Bernoulli Distribution

Let $X$ represent a binary event with probability, $\mathbb{P}(X=1) = p$ and $\mathbb{P}(X=0)=1-p$ for some $p\in[0,1]$. We say that $X\sim \operatorname{Bernoulli}(p)$ has a Bernoulli distribution and 

$$
f(x)=
p^{x}(1-p)^{1-x},~\text{for}~x\in\{0, 1\}.
$$

---

### Binomial Distribution

Suppose we have a binary event with probability $p\in[0,1]$ and consider a sequence of $n$ independent events, the pmf is

$$
f(x) =
\begin{cases}
{n \choose x} p^{x}(1-p)^{n-x} & \text{for}~x=0,\ldots, n\\
0 & \text{otherwise}. 
\end{cases}
$$

A random variable with this pmf is a Binomial random variable $X\sim \operatorname{Binomial}(n, p)$. 

The variables $n$ and $p$ are fixed parameters, whereas $x$ is a particular value of the random variable.

---

Steph Curry's career free throw percentage is 92%. What is the chance that if Steph shoots 4 free throws, he will make exactly 3 of them?

::: {.notes}
$p=.92$, $1-p=.08$, $n=4$, $x=3$

$$
P(x=3) = {4\choose 3} (0.92)^3 \cdot (0.08)^{4-1} \approx 0.2492.
$$
:::

---

Let's show a simulation of 1000 Steph Curry free throws.

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom

# Parameters
n = 1000
p = 0.9
num_simulations = 10000

# Create a random number generator instance
rng = np.random.default_rng(12345)

# Simulate binomial trials using the RNG
data = rng.binomial(n=n, p=p, size=num_simulations)

# Exact binomial PMF
x = np.arange(binom.ppf(0.01, n, p),
              binom.ppf(0.99, n, p))
pmf = binom.pmf(x, n, p)

# Plot histogram and true distribution
plt.figure(figsize=(10, 6))
plt.hist(data, bins=30, edgecolor='black', density=True)
plt.plot(x, pmf, 'o', color='r', label='Exact Binomial PMF')
plt.title(f'Histogram of Binomial Trials (n={n}, p={p})')
plt.xlabel('Number of Successes')
plt.ylabel('Frequency (Normalized)')
plt.grid(True)
plt.legend()
plt.show()
```

---

### Geometric Distribution

A random variable $X$ has a geometric distribution $X\sim \operatorname{Geom}(p)$ with parameter $p\in(0,1)$ if

$$
\mathbb{P}(X=k) = p(1-p)^{k-1}, \quad k\geq 1.
$$

Think of this as the probability distribution of the number of failures before the first success. 

---

In the game Settlers of Catan you roll two dice and the number indicates which resource you can pick up. Let success be defined as rolling a 9, which allows you to pick up the resource ore. What is the probability that it will take more than 10 rolls to roll a 9?


::: {.notes}
$p = \frac{4}{36}$

$$
P(X>10) = (1-p)^{10} = \frac{8/9}^{10} \approx 0.314
$$
:::

--- 

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import geom

# Parameters
n = 1000
p = 4.0/36
num_simulations = 10000

# Create a random number generator instance
rng = np.random.default_rng(12345)

# Simulate binomial trials using the RNG
data = rng.geometric(p=p, size=num_simulations)

# Exact binomial PMF
x = np.arange(geom.ppf(0.01, p),
              geom.ppf(0.99, p))
pmf = geom.pmf(x, p)

# Plot histogram and true distribution
plt.figure(figsize=(10, 6))
plt.hist(data, bins=30, edgecolor='black', density=True)
plt.plot(x, pmf, 'o', color='r', label='Exact Geometric PMF')
plt.title(f'Histogram of Trials (n={n}, p={p:0.2f})')
plt.xlabel('Number of rolls before success')
plt.ylabel('Frequency (Normalized)')
plt.grid(True)
plt.legend()
plt.show()
```

---

### Poisson Distribution

A random variable $X$ has a Poisson distribution $X\sim \operatorname{Poisson}(\lambda)$, with parameter $\lambda$ if

$$
f(x) = e^{-\lambda}\frac{\lambda^{x}}{x!} \quad x\geq 0.
$$

The Poisson distribution is often used as a model for counts of rare events like radioactive decay, traffic accidents, and horse kicks to the head.

---

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import poisson

# Set seaborn style
sns.set(style="whitegrid")

# Define lambda values
lambda_values = range(1, 7)

# Define x range
x = np.arange(0, 20)

# Create plot
plt.figure(figsize=(10, 6))
for lam in lambda_values:
    pmf = poisson.pmf(x, mu=lam)
    plt.plot(x, pmf, marker='o', label=f'λ = {lam}')

# Set integer ticks on x-axis
plt.xticks(np.arange(0, 25, 1))
plt.grid(True, which='both', axis='x', linestyle='--', linewidth=0.5)


# Add labels and title
plt.title('Poisson Distributions for λ from 1 to 6', fontsize=14)
plt.xlabel('Number of Events (k)', fontsize=12)
plt.ylabel('Probability Mass Function (PMF)', fontsize=12)
plt.legend(title='Lambda (λ)')
plt.tight_layout()

plt.show()
```

---

A pizza restaurant receives an average of 20 delivery orders per hour. Assuming that this rate is independent and random. What is the probability of receiving exactly 15 orders in a given hour? What is the probability of receiving more than 25 orders?

::: {.notes}
$$
\begin{align}
P(X>25) &= 1 - P(X\leq 25) = \\
&= 1 - \sum_{k=1}^{25}P(X=k) \\
&= 1 - \sum_{k=1}^{25}e^{-20}20^{k}/(k!) \approx 1-0.894 \approx .106
\end{align}
$$

Roughly 10 percent.
:::

---

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson

# Parameters
lam=20
num_simulations = 10000

# Create a random number generator instance
rng = np.random.default_rng(12345)

# Simulate binomial trials using the RNG
data = rng.poisson(lam=lam, size=num_simulations)

# Exact binomial PMF
x = np.arange(poisson.ppf(0.01, lam),
              poisson.ppf(0.99, lam))
pmf = poisson.pmf(x, lam)

# Plot histogram and true distribution
plt.figure(figsize=(10, 6))
plt.hist(data, bins=30, edgecolor='black', density=True)
plt.plot(x, pmf, 'o', color='r', label='Exact Poisson PMF')
plt.title(f'Histogram of Trials ($\\lambda$ = {lam:.2f})')
plt.xlabel('Number of orders per hour')
plt.ylabel('Frequency (Normalized)')
plt.grid(True)
plt.legend()
plt.show()
```

## Some Important Continuous Random Variables

### Uniform Distribution

The random variable is uniformly distributed $X\sim \operatorname{Uniform}(a,b)$ on the interval $[a,b]$ ($a<b$) if

$$
f(x) =
\begin{cases}
\frac{1}{b-a} & \text{for}~x\in[a,b] \\
0 & \text{otherwise}.
\end{cases}
$$

---

The CDF is

$$
F(x) =
\begin{cases}
0 & x<a \\
\frac{x-a}{b-a} & x\in[a,b] \\
1 & x>b.
\end{cases}
$$

---

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import uniform

# Set seaborn style
sns.set(style="whitegrid")

# Define the range and distribution
lower, upper = -1, 3
x = np.linspace(lower - 1, upper + 1, 500)
dist = uniform(loc=lower, scale=upper - lower)

# Calculate PDF and CDF
y_pdf = dist.pdf(x)
y_cdf = dist.cdf(x)

# Create side-by-side plots
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# PDF plot
axes[0].plot(x, y_pdf, color='blue')
axes[0].set_title('PDF of Uniform Distribution [-1, 3]', fontsize=14)
axes[0].set_xlabel('x', fontsize=12)
axes[0].set_ylabel('Probability Density', fontsize=12)

# CDF plot
axes[1].plot(x, y_cdf, color='green')
axes[1].set_title('CDF of Uniform Distribution [-1, 3]', fontsize=14)
axes[1].set_xlabel('x', fontsize=12)
axes[1].set_ylabel('Cumulative Probability', fontsize=12)

plt.show()
```

---

### Normal (Gaussian)

The random variable $X$ has a Normal (Gaussian)  distribution $X\sim N(\mu, \sigma^{2})$ with parameters $\mu\in\mathbb{R}$ and $\sigma>0$ if

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}}\operatorname{exp}\left\{-\frac{1}{2\sigma^{2}}(x-\mu)^{2} \right\},\quad x\in\mathbb{R}.
$$

The parameter $\mu$ is the mean of the distribution and $\sigma$ is the standard deviation.

$X$ has a standard normal distribution if $\mu=0$ and $\sigma=1$.

---

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set seaborn style
sns.set(style="whitegrid")

# Define parameters
mean = 0
variances = [0.5, 1, 2, 5]
x = np.linspace(-10, 10, 1000)

# Create plot
plt.figure(figsize=(10, 6))
colors = sns.color_palette("plasma", len(variances))

for var, color in zip(variances, colors):
    std_dev = np.sqrt(var)
    y = (1 / (std_dev * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std_dev) ** 2)
    plt.plot(x, y, label=f"Variance = {var}", color=color)

plt.title("Gaussian Distributions with Mean = 0 and Varying Variances", fontsize=14)
plt.xlabel("x", fontsize=12)
plt.ylabel("Probability Density", fontsize=12)
plt.legend(title="Distributions")
plt.tight_layout()

plt.show()
```

---

### Exponential Distribution

The random variable $X$ has an exponential distribution $X\sim \operatorname{Exp}(\beta)$ with parameter $\beta>0$ if

$$
f(x) = \frac{1}{\beta}e^{-x/\beta},\quad x>0.
$$

The exponential distribution is used to model the lifetime of electronic components and the waiting times between rare events.

---

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set seaborn style for aesthetics
sns.set(style="whitegrid")

# Define beta values
beta_values = [0.5, 1, 2, 5, 10]
x = np.linspace(0, 10, 1000)

# Create the plot
plt.figure(figsize=(10, 6))
for beta in beta_values:
    y = (1/beta) * np.exp(-x/beta)
    plt.plot(x, y, label=f"β = {beta}")

# Customize plot
plt.title("Exponential Distributions for Different β Values", fontsize=16)
plt.xlabel("x", fontsize=14)
plt.ylabel("Probability Density", fontsize=14)
plt.legend(title="Beta Values")
plt.tight_layout()

plt.show()
```

---

An electronic component has a constant failure rate of 10% per year. Assuming the time until failure follows an exponential distribution, what is the probability that the part lasts more than 5 years?  

::: {.notes}
$\beta = 10$

$$
P(T>5) = \int_{5}^{\infty}\frac{1}{10}e^{-s/10}ds = e^{-0.1\cdot 5} \approx 0.605
$$

60 percent chance it lasts more than 5 years.
:::

---

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import expon

# Parameters
beta=0.1
num_simulations = 100000

# Create a random number generator instance
rng = np.random.default_rng(12345)

# Simulate binomial trials using the RNG
data = rng.exponential(scale=1/beta, size=num_simulations)

# Exact binomial PMF
x = np.arange(expon.ppf(0.01, scale=1/beta),
              expon.ppf(0.99, scale=1/beta))
pmf = expon.pdf(x, beta)

# Plot histogram and true distribution
plt.figure(figsize=(10, 6))
plt.hist(data, bins=30, edgecolor='black', density=True)
plt.plot(x, pmf, 'o', color='r', label='Exact Exponential PDF')
plt.title(f'Histogram of Trials ($\\beta$ = {beta:.2f})')
plt.xlabel('Simulations of part failure')
plt.ylabel('Frequency')
plt.grid(True)
plt.legend()
plt.show()
```

---

### Gamma Distribution

The gamma function is defined by

$$
\Gamma(\alpha) = \int_{0}^{\infty}y^{\alpha-1}e^{-y}dy.
$$

The gamma distribution for a random variable $X\sim \operatorname{Gamma}(\alpha, \beta)$ with $\alpha,\beta >0$ is defined by

$$
f(x) = \frac{1}{\beta^{\alpha}\Gamma(\alpha)}x^{\alpha-1}e^{-x/\beta}.
$$

The exponential distribution is a $\operatorname{Gamma}(1,\beta)$ distribution.

---

### t Distribution

$X$ has a $t$ distribution with $\nu$ degrees of freedom, $X\sim t_{\nu}$ if

$$
f(x) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)} \frac{1}{\left(1 + \frac{x^{2}}{\nu}\right)^{(\nu +1)/2}}.
$$

The $t$-distribution is similar to the Normal but has thicker tails at the end of the distribution.

---

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import t
import os

# Set seaborn style
sns.set(style="whitegrid")

# Define degrees of freedom and t-values
dfs = [1, 5, 10, 30]
t_values = np.linspace(-5, 5, 500)

# Plot t-distributions
plt.figure(figsize=(10, 6))
for df in dfs:
    plt.plot(t_values, t.pdf(t_values, df), label=f'$\\nu$={df}')

plt.title('t-Distribution for Various Degrees of Freedom', fontsize=14)
plt.xlabel('t-value', fontsize=12)
plt.ylabel('Probability Density', fontsize=12)
plt.legend(title='Degrees of Freedom')
plt.tight_layout()
plt.show()
```

---

### Cauchy Distribution

The Cauchy distribution is a $t$ distribution with $\nu=1$. The density is given by

$$
f(x) = \frac{1}{\pi(1+x^{2})}.
$$


## Bivariate Distributions

Given a pair of discrete random variables $X$ and $Y$, the joint mass function is defined as

$$
f(x, y) = \mathbb{P}(X=x, Y=y).
$$

We can be more specific and write $f_{X,Y}$.

---

In the continuous case, a pdf for the random variable $(X, Y)$ is a function $f(x, y)$ such that

1. $f(x, y) \geq 0$ for all $(x, y)$,
1. $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y) dxdy = 1$ and,
1. for any set $A\subset \mathbb{R}\times \mathbb{R}$, $\mathbb{P}((X,Y)\in A) = \int\int_{A}f(x,y)dxdy$.

In the discrete or continuous case the joint CDF is

$$
F_{X,Y}(x,y) = \mathbb{P}(X\leq x, Y\leq y).
$$

## Marginal Distributions

If $(X,Y)$ have joint distribution with mass function $f_{X,Y}$, then the *marginal mass function* for $X$ is defined by

$$
f_{X}(x) = \sum_{y}\mathbb{P}(X=x, Y=y) = \sum_{y}f(x,y).
$$

and the *marginal mass function* for $Y$ is

$$
f_{Y}(y) = \sum_{x}\mathbb{P}(X=x, Y=y) = \sum_{x}f(x,y).
$$

---

For continuous random variables, the marginal density functions are

$$
f_{X}(x) = \int f(x,y)dy, \quad\text{and}\quad f_{Y}(y) = \int f(x,y)dx.
$$

The corresponding marginal distribution functions are $F_{X}$ and $F_{Y}$.

## Independent Random Variables

Two random variables $X$ and $Y$ are *independent* if for every $A$ and $B$,

$$
\mathbb{P}(X\in A, Y\in B) = \mathbb{P}(X\in A)\mathbb{P}(Y\in B).
$$

If this condition is not satisfied then $X$ and $Y$ are *dependent*.

---

Let $X$ and $Y$ have joint PDF $f_{X,Y}$, then $X$ and $Y$ are independent if and only if 

$$
f_{X,Y}(x,y) = f_{X}(x)f_{Y}(y),
$$

for all values $x$ and $y$.

## Conditional Distributions

For discrete random variables, the *conditional probability mass function* is

$$
\small
f_{X\vert Y}(x\vert y) = \mathbb{P}(X=x\vert Y=y) = \frac{\mathbb{P}(X=x, Y=y)}{\mathbb{P}(Y=y)} = \frac{f_{X,Y}(x,y)}{f_{Y}(y)},
$$

if $f_{Y}(y)>0$.

---

For continuous random variables, the *conditional probability density function* is

$$
f_{X\vert Y}(x\vert y) = \frac{f_{X,Y}(x,y)}{f_{Y}(y)}
$$

assuming that $f_{Y}(y)>0$. Then,

$$
 \mathbb{P}(X\in A\vert Y=y) = \int_{A}f_{X\vert Y}(x\vert y)dx.
$$

## Multivariate Distributions and IID Samples

Given $n$ random variables $X_{i}$, the *random vector* is defined as $X=(X_{1},\ldots, X_{n})$.

Let $f(x_{1},\ldots, x_{n})$ denote the multivariate PDF. The random variables are independent if

$$
f(x_{1},\ldots, x_{n}) = \prod_{i=1}^{n}f_{X_{i}}(x_{i}).
$$

---

If $X_{1},\ldots, X_{n}$ are independent and each has the same marginal distribution with CDF $F$, we say that $X_{1},\ldots X_{n}$ are independent and identically distributed (IID) and we write

$$
X_{1},\ldots X_{n}\sim F.
$$

If $F$ has density $f$ we also write $X_{1},\ldots X_{n}\sim f$. The variables $X_{1},\ldots X_{n}$ are called a *random sample* of size $n$ from $F$.

## The Multivariate Normal

A random vector $X = (X_{1},\ldots X_{n})$ has a multivariate normal distribution $X\sim N(\mu, \Sigma)$ if

$$
\small
f(\mathbf{x}; \mu, \Sigma) = \frac{1}{(2\pi)^{k/2}\vert \Sigma \vert^{1/2}}\operatorname{exp}\left\{-\frac{1}{2}(\mathbf{x}-\mu)^{T}\Sigma^{-1}(\mathbf{x}-\mu) \right\},
$$

where $\mu\in\mathbb{R}^{n}$ is the vector of means of each $X_{i}$, $\Sigma\in\mathbb{R}^{n\times n}$ is the symmetric positive definite covariance matrix, and $\vert \Sigma \vert$ is the determinant of $\Sigma$.

---

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import multivariate_normal
from mpl_toolkits.mplot3d import Axes3D

# Set seaborn style
sns.set(style="white")

# Mean and covariance matrix
mean = [0, 0]
cov = [[1, 0.5], [0.5, 1]]

# Create grid
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
pos = np.dstack((X, Y))

# Multivariate normal distribution
rv = multivariate_normal(mean, cov)
Z = rv.pdf(pos)

# Create figure and subplots
fig = plt.figure(figsize=(14, 6))
fig.tight_layout(pad=0.5)


# 2D Contour plot
ax1 = fig.add_subplot(1, 2, 1)
contour = ax1.contour(X, Y, Z, cmap='viridis')
ax1.set_title('2D Contour Plot')
ax1.set_xlabel('X')
ax1.set_ylabel('Y')
ax1.set_xlim(-3, 3)
ax1.set_ylim(-3, 3)

# 3D Surface plot
ax2 = fig.add_subplot(1, 2, 2, projection='3d')
surf = ax2.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')
ax2.set_title('3D Surface Plot')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.set_zlabel('Probability Density')

plt.show()
```


## Transformations of Random Variables

Suppose that $X$ is a random variable with PDF $f_{X}$ and CDF $F_{X}$. Let $Y=r(X)$ be a function of $X$. 

The new variable $Y$ is a *transformation* of $X$. 

How do we compute the PDF and CDF of $Y$?

---

To find $f_{Y}$ follow these three steps:

1. For each $y$, find the set $A_{y}=\{x:~r(x)\leq y\}$.
1. Find the CDF
$$
\small
\begin{align}
F_{Y}(y) &= \mathbb{P}(Y\leq y) =  \mathbb{P}(r(X)\leq y) \\
&= \mathbb{P}(\{x:~r(x)\leq y\}) \\
&= \int_{A_{y}}f_{X}(x)dx.
\end{align} 
$$
1. The PDF is $f_{Y}(y) = F^{\prime}_{Y}(y)$.

The steps for functions of several random variables are similar.

---

### Example

Let $f_{X}(x)=e^{-x}$ for $x>0$. Let $Y=r(x)=\log(x)$. Determine $F_Y(y)$ and $f_{Y}(y)$.

::: {.notes}
We have that $F_{X}(x) = \int_{0}^{x}e^{-s}ds = 1-e^{-x}$.

The set $A_{y} = \{ x: x\leq e^{y} \}$ why?

Then

$$
\begin{align}
F_{Y}(y) &= \mathbb{P}(Y\leq y) = \mathbb{P}(\log(x)\leq y) \\
&= \mathbb{P}(X\leq e^{y}) = F_{X}(e^{y})= 1- e^{-e^{y}} \\
\end{align}
$$

Taking derivatives (FTC) we get

$$
f_{Y}(y) = e^{y}e^{-e^{y}}
$$
:::


## Summary

- Random variables
- CDFs
- PMFs and important discrete distributions
- PDFs and important continuous distributions
- Bivariate (joint) and marginal distributions
- Conditional distributions
- the Multivariate Normal distribution
- Transformations of random variables