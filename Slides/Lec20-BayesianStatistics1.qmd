---
title: "Bayesian Statistics 1"
jupyter: python3
---

## Lecture Overview

The Bayesian interpretation of probability is that probability is subjective and reflects personal belief, updated by data. 

The probability of an event quantifies a degree of belief in a hypothesis.

Bayesian statistics uses probability to quantify uncertainty about unknowns and updates prior beliefs to posterior beliefs using Bayes' theorem as new data arrive.

In this lecture we examine how we can use this interpretation to solve interesting problems.

## Lecture Objectives

- Review Bayes' Theorem
- Bayes' Theorem to update probabilities
- Bayes' Theorem for distributions

## Bayes' Theorem

Let $A$ and $B$ be events such that $\mathbb{P}(B)>0$, then

$$
\mathbb{P}(A\mid B) = \frac{\mathbb{P}(B\mid A)\mathbb{P}(A)}{\mathbb{P}(B)}.
$$


## Terminology

The probability

- $\mathbb{P}(A\mid B)$ is called the posterior
- $\mathbb{P}(A)$ is called the prior
- $\mathbb{P}(B\mid A)$ is called the likelihood
- $\mathbb{P}(B)$ total probability or evidence

This formula tells us that $\mathbb{P}(A\mid B)\propto \mathbb{P}(B\mid A)\mathbb{P}(A)$.

## Cookies

Suppose there are two bowls of cookies:

- The first bowl contains 30 vanilla cookies and 10 chocolate cookies.
- The second contains 20 vanilla cookies and 20 chocolate cookies.

:::: {.fragment}
__Question:__ If you choose a bowl at random, and then grab a cookie at random and get a vanilla cookie, what is the probability it came from the first bowl?

::::

::: {.notes}
Intuitively, we know that the first bowl had more vanilla cookies, so we should expect it to be more than a 50% chance.
:::

---

First define the experiment:

- choose a bowl at random
- draw one cookie at random

The sample space space is

$$
\Omega = \{ (B_{1}, V), (B_{1},C), (B_{2}, V), (B_{2}, C)\},
$$

where $B_{1}$ and $B_{2}$ are bowls 1 and 2, respectively, and $V$ and $C$ are vanilla or chocolate cookies, respectively.

---

We can then consider events

- $A = \{ (B_{1},V), (B_{1},C)\}$, the event the cookie came from bowl 1
- $B = \{ (B_{1},V), (B_{2},V)\}$, the event the cookie is vanilla


:::: {.fragment}
In words, what does $\mathbb{P}(A\mid B)$ represent?
::::

::: {.notes}
Rewrite these as

$$
\mathbb{P}(\text{Bowl 1} \vert \text{Vanilla}).
$$
:::

---

We now have three probabilities we need to determine:

- $\small \mathbb{P}(\text{Bowl 1})$ which is one bowl out of two, is  1/2.
- $\small \mathbb{P}(\text{Vanilla} \vert \text{Bowl 1})$ which is 30 out of 40 cookies, is 3/4.
- $\small \mathbb{P}(\text{Vanilla})$ is a little tougher. It's the combined probability of Vanillas from either bowl. What can we do?

::: {.notes}
$$\small \begin{align*}
 & \mathbb{P}(\text{Vanilla})  = \\
 & = \mathbb{P}(\text{Vanilla} \vert \text{Bowl 1})\;\mathbb{P}(\text{Bowl 1}) +  \mathbb{P}(\text{Vanilla} \vert \text{Bowl 2})\mathbb{P}(\text{Bowl 2}) \\
& = (3/4)(1/2)+(1/2)(1/2) =5/8 
\end{align*}$$
:::

---

Using that

- $\small \mathbb{P}(\text{Bowl 1}) = 1/2$,
- $\small \mathbb{P}(\text{Vanilla} \vert \text{Bowl 1}) = 30/40$, and
- $\small \mathbb{P}(\text{Vanilla}) = 5/8$,

we have

$$
\small \begin{align*}
\mathbb{P}(\text{Bowl 1} \vert \text{Vanilla})\ & = \frac{\mathbb{P}(\text{Vanilla} \vert \text{Bowl 1})\mathbb{P}(\text{Bowl 1})}{\mathbb{P}(\text{Vanilla})}\\
& = \frac{(3/4)(1/2)}{5/8} = 3/5.
\end{align*}
$$

## Bayesian Concepts

The main way Bayes' Rule is used in Bayesian statistics is as a way to update the probability of a hypothesis $H$, given some data $D$.

For $\small \mathbb{P}(D)>0$,

$$
\small \mathbb{P}(H \,\vert\, D)\; =  \frac{\mathbb{P}(D\,\vert\,H)\mathbb{P}(H) \; }{\mathbb{P}(D)}. 
$$

---

:::: {.incremental}
- $\mathbb{P}(H)$ is the probability of the hypothesis before we see the data, called the **prior** probability.
- $\mathbb{P}(H \vert D)$ is the probability of the hypothesis after we see the data, called the **posterior** probability.
- $\mathbb{P}(D \vert H)$ is the probability of the data under the hypothesis, called the **likelihood**.
- $\mathbb{P}(D)$ is the **total probability of the data** under any hypothesis (it is also known as the **normalizing constant**).
::::

## Prior 

The *prior* is often the trickiest portion of the Bayes' Rule to pin down. 

Sometimes it can be computed exactly as in the cookie bowl problem, where we were equally likely to pick each bowl. 

:::: {.fragment}
But what if we chose bowls proportional to their size? We would need to include that into our analysis. 
::::

:::: {.fragment}
Other times, people might disagree about which background information is relevant to the problem at hand. 
::::


## Likelihood

The *likelihood* is usually well defined and can be computed directly. In the cookie problem, we know the numbers of different cookies in each bowl, so we can compute the probabilities under each hypothesis.


## Total Probability

Determining the *total probability* of the data is often a lot more difficult than you might expect because we often don't know every possible hypothesis. 

However, usually the goal is to pin down a set of *mutually exclusive* and *collectively exhaustive* hypotheses. 

::: {.notes}
This means a set of hypotheses where only one of them can be true (if H1 is true, H2 cannot be true), and one of them must be true (taken together the hypotheses cover all possible cases).

Cookies: 

- H1 - pick bowl1, H2 pick bowl2 (exclusive) only 1 true
- One of H1 or H2 has to happen (exhaustive) one must be true
:::


Over a set of *i* hypotheses, we say:

$$
\small \mathbb{P}(D) = \sum_i{P(H_i)P(D \vert H_i)}. 
$$

## Bayesian Updates

A Bayesian update is the process of generating a posterior probability from a prior probability using data.

:::: {.fragment}
Is there a way to keep track of this process?
::::


## Bayes Table

A Bayes table is table that keeps track of the probabilities of all hypotheses as we update them using our data.

Let's go step by step to create a Bayes table for the cookie problem. First, we need a table that contains all possible hypotheses, one row for each hypothesis:

```{python}
#| echo: true
import pandas as pd

table = pd.DataFrame(index=['Bowl1', 'Bowl2'])
table
```

---

Next, we add the prior probabilities of each hypothesis. Our prior is that it was equally likely to get a vanilla cookie from either bowl.

::: {.notes}
Link up $H_{1}$ notation and $
:::

```{python}
#| echo: true
table['prior'] = [1/2, 1/2]
table
```

---

The likelihood of each hypothesis is the fraction of cookies in each bowl that is vanilla. For example, 

$$
\small \mathbb{P}(D\,\vert\,\text{Bowl 1}) = \frac{30}{40} = 0.75.
$$

```{python}
#| echo: true
table['likelihood'] = [30/40, 20/40]
table
```

---

Next, let's compute the *unnormalized posteriors*. This is just the prior multiplied by the likelihood.

```{python}
#| echo: true
table['unnormalized'] = table['prior'] * table['likelihood']
table
```

---

The final missing piece is to divide by the total probability of the data. 

What we are doing is *normalizing* the posteriors so that they sum up to 1.

To find the total probability of the data we directly sum over the unnormalized posteriors:

```{python}
#| echo: true
prob_data = table['unnormalized'].sum()
prob_data
```

This gives us 5/8, just like we calculated before. 

---

Finally, we can use the total probability of the data to get the posterior probability of each hypothesis.

```{python}
#| echo: true
table['posterior'] = table['unnormalized'] / prob_data
table
```

::: {.notes}
The posterior probability of the first bowl, given that we observed a vanilla cookie, is 0.6, the same as when we used Bayes' theorem directly before. 

Notice that the posteriors add up to 1 (as we should expect given mutually exclusive and collectively exhaustive hypotheses).
:::

## Monty Hall
Recall from homework our Monty Hall problem.

Let's use the Bayesian framework to understand this problem.

::: {.notes}
Introduce the setup with the doors,
we pick door 1, monty open's door 3.
:::

---

Each door starts with an equal prior probability of holding the prize.

```{python}
#| echo: true
table = pd.DataFrame(index=['Door 1', 'Door 2', 'Door 3'])
table['prior'] = 1/3, 1/3, 1/3
table
```

:::: {.fragment}
What is our *data* in this scenario? 
::::

:::: {.fragment}
We must determine:

- what are the *hypotheses*?
- what is the *likelihood* of the data under each hypothesis? 
::::

::: {.notes}
Data is hosts actions (causes us to update our prior beliefs)

Without loss of generality, suppose we originally picked door 1. Now Monty opens a door (let's say door 3, again without loss of generality) to reveal a gag prize. So the door that is open gives us the data.
:::

---

Hypothesis 1: The prize is behind door 1. What is $\mathbb{P}(D\mid H_{1})$?

Hypothesis 2: The prize is behind door 2. What is $\mathbb{P}(D\mid H_{2})$?

Hypothesis 3: The prize is behind door 3. $\mathbb{P}(D\mid H_{3})$?


::: {.notes}
In this case Monty chose door 2 or door 3 at random, so he was equally likely to open door 2 and 3, so the observation that he opened door 3 had a 50/50 chance of occurring.

In this case Monty *must* open door 3, so the observation that he opened door 3 was guaranteed to happen.

Monty could not have opened a door with the prize behind it, so the probability of seeing him open door 3 under this hypothesis is 0.
:::

---

The likelihoods are:

- $\mathbb{P}(D\mid H_{1}) = 0.5$
- $\mathbb{P}(D\mid H_{2}) = 1$
- $\mathbb{P}(D\mid H_{3}) = 0$

```{python}
#| echo: true
table['likelihood'] = [1/2, 1, 0]
table
```

---

The posterior update gives

```{python}
#| echo: true
table['unnormalized'] = table['prior'] * table['likelihood']
prob_data = table['unnormalized'].sum()
table['posterior'] = table['unnormalized'] / prob_data
table
```

::: {.notes}
Turns out there is a 2/3 probability the prize is behind door 2! We should switch doors.
:::

## Prior and Posterior Distributions

The set of prior probabilities are in reality a *prior distribution*. Likewise, the set of *posterior probabilities* are in reality a *posterior distribution* across hypotheses. 

Let's reformulate the cookie problem using distributions.

We will use a uniform probability distribution as a prior.

::: {.notes}
Called a uniform prior.
:::

---

Our starting distribution:

```{python}
#| echo: true
import numpy as np
from scipy.stats import randint
dist = randint(1, 3)

distribution = pd.DataFrame(index=['Bowl 1', 'Bowl 2'])
#uniform prior distribution
distribution['prior'] = dist.pmf(np.arange(1,3)) 
distribution
```

---

```{python}
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
distribution.plot(kind = 'bar', ax = ax, ylim = [0, 1], legend = False, ylabel = 'Probability', fontsize = 16)
plt.xticks(rotation='horizontal')
ax.yaxis.label.set_fontsize(16)
plt.title('Prior Distribution (Uniform Prior)', size = 16)
plt.show()
```

--- 

We can then compute the posterior probabilities as before.

```{python}
distribution['likelihood'] = [0.75, 0.5]
unnormalized = distribution['prior'] * distribution['likelihood']
prob_data = unnormalized.sum()
distribution['posterior'] = unnormalized / prob_data
distribution
```

---

The plotted posterior distribution is below.

```{python}
fig, ax = plt.subplots()
distribution['posterior'].plot(kind = 'bar', ax = ax, ylim = [0, 1], legend = False, ylabel = 'Probability', fontsize = 16)
plt.xticks(rotation='horizontal')
ax.yaxis.label.set_fontsize(16)
plt.title('Posterior Distribution', size = 16)
plt.show()
```

## Summary 

In this lecture we covered

- Bayes' theorem
- Bayesian interpretation of
    - Cookie selection
    - Monty Hall problem
- Linked Bayesian updates to updating distributions.
