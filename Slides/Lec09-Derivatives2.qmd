---
title: "Derivatives II"
jupyter: python3
---

## Lecture Overview

Today we continue our review of derivatives. In particular, derivatives of vector-valued and functions of matrices.

These concepts are essential for understanding back-propagation in deep neural networks.

We also introduce the multivariate version of Taylor's theorem which will be useful in our lectures on optimization.

## Lecture Objectives

Review:

- Derivatives of vector-valued functions  
- Derivatives of matrix-valued functions
- Automatic Differentiation
- Multivariate Taylor's theorem

## Increasing/Decreasing Test

The derivative $f^{\prime}(x)$ gives us information about the function $f(x)$.

1. The function $f(x)$ is increasing when $f^{\prime}(x)>0$.
1. The function $f(x)$ is decreasing when $f^{\prime}(x)<0$.

A critical point is a value $x_{0}$ where $f^{\prime}(x_{0})=0$.

---

**Example:** $f(x)=x^{2}-1$

```{python}
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt

# Define the function and its derivative
def f(x):
    return x**2 - 1

def df(x):
    return 2*x

x = np.linspace(-3, 3, 400)
y = f(x)
dy = df(x)

# Find where derivative is positive (increasing) and negative (decreasing)
increasing = dy > 0
decreasing = dy < 0

plt.figure(figsize=(8, 5))
plt.plot(x, y, label='$f(x) = x^2 - 1$')
plt.fill_between(x, y, where=increasing, color='green', alpha=0.3, label='Increasing')
plt.fill_between(x, y, where=decreasing, color='red', alpha=0.3, label='Decreasing')
plt.axhline(0, color='gray', lw=0.5)
plt.axvline(0, color='gray', lw=0.5)
plt.legend()
plt.title('Increasing and Decreasing Regions of $f(x) = x^2 - 1$')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.show()

```

## First Derivative Test

The first derivative test gives us a way to determine whether a critical point is a local maximum, minimum, or neither.

If $f^{\prime}(x_0) = 0$ and $f'(x)$ changes sign at $x_0$:

- If $f^{\prime}(x)$ changes from positive to negative at $x_0$, then $f(x_0)$ is a local maximum.
- If $f'(x)$ changes from negative to positive at $x_0$, then $f(x_0)$ is a local minimum.
- If $f'(x)$ does not change sign at $x_0$, then $f(x_0)$ is not a local extremum.

This test is useful for analyzing the behavior of functions near their critical points.

---

**Example:** $f(x)=x^{2}-1$

- What is the critical point $x_{0}$?
- What is the sign change as we pass from negative to positive $x$ values through $x_{0}$?


## Convexity/Concavity Test

The convexity or concavity of a function can be determined using the second derivative.

- If $f^{\prime\prime}(x) > 0$ for all $x$ in an interval, then $f(x)$ is **convex** (curves upwards) on that interval.
- If $f^{\prime\prime}(x) < 0$ for all $x$ in an interval, then $f(x)$ is **concave** (curves downwards) on that interval.

---

**Example:** $f(x)=x^{2}-1$

- What is $f^{\prime\prime}(x)$? 
- On what intervals is this positive or negative?


## Second Derivative Test

The second derivative test helps determine the nature of a critical point $x_0$ for a function $f(x)$:

- If $f^{\prime}(x_0) = 0$ and $f^{\prime\prime}(x_0) > 0$, then $f(x_0)$ is a **local minimum**.
- If $f^{\prime}(x_0) = 0$ and $f^{\prime\prime}(x_0) < 0$, then $f(x_0)$ is a **local maximum**.
- If $f^{\prime}(x_0) = 0$ and $f^{\prime\prime}(x_0) = 0$, the test is **inconclusive**.

---

**Example:** $f(x)=x^{2}-1$

- Using the second derivative test, what can we say about the minimum?


---

**Example:** $f(x)=x^{3}$

- What is $f^{\prime}(x)$?
- What is the critical point $x_{0}$?
- What is $f^{\prime\prime}(x_{0})$?


## Gradient

Given a real-valued function $f(\mathbf{x})$ with $\mathbf{x}\in\mathbb{R}^{n}$, the gradient is a vector consisting of all partial derivatives of $f$:

$$
\nabla f(\mathbf{x}) = 
\begin{bmatrix}
\frac{\partial}{\partial x_{1}}f(\mathbf{x}) \\
\frac{\partial}{\partial x_{2}}f(\mathbf{x}) \\
\vdots \\
\frac{\partial}{\partial x_{n}}f(\mathbf{x}) \\
\end{bmatrix}.
$$

---

**Example** $f(x_{1},x_{2})= \sin(x_{1})\cos(x_{2})$

Calculate $\nabla f(x_{1},x_{2})$:

::: {.notes}
$$
\begin{align*}
f(x_{1}, x_{2}) &= \sin(x_{1})\cos(x_{2}) \\
\frac{\partial f}{\partial x_{1}} &= \cos(x_{1})\cos(x_{2}) \\
\frac{\partial f}{\partial x_{2}} &= -\sin(x_{1})\sin(x_{2}) \\
\nabla f(x_{1}, x_{2}) &= 
\begin{bmatrix}
\cos(x_{1})\cos(x_{2}) \\
-\sin(x_{1})\sin(x_{2})
\end{bmatrix}
\end{align*}
$$
:::

## Hessian

Given a real-valued function $f(\mathbf{x})$ with $\mathbf{x}\in\mathbb{R}^{n}$, the *Hessian* is the symmetric matrix:

$$
\small
\nabla^{2} f(\mathbf{x}) = 
\begin{bmatrix}
\frac{\partial^{2}}{\partial x_{1}\partial x_{1}}f(\mathbf{x}) &  \frac{\partial^{2}}{\partial x_{1}\partial x_{2}}f(\mathbf{x}) & \cdots & \frac{\partial^{2}}{\partial x_{1}\partial x_{n}}f(\mathbf{x}) \\
\frac{\partial^{2}}{\partial x_{2}\partial x_{1}}f(\mathbf{x}) &  \frac{\partial^{2}}{\partial x_{2}\partial x_{2}}f(\mathbf{x}) & \cdots & \frac{\partial^{2}}{\partial x_{2}\partial x_{n}}f(\mathbf{x})\\
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial^{2}}{\partial x_{n}\partial x_{1}}f(\mathbf{x}) &  \frac{\partial^{2}}{\partial x_{n}\partial x_{2}}f(\mathbf{x}) & \cdots & \frac{\partial^{2}}{\partial x_{n}\partial x_{n}}f(\mathbf{x})\\
\end{bmatrix}.
$$

---

**Example:** $f(x_{1},x_{2})=x_{1}^{3} + x_{1}^{2}x_{2} - x_{1}x_{2}^{2} + x_{2}^{3}$.

Calculate $\nabla^{2} f(x_{1},x_{2})$:

::: {.notes}
$$
\begin{align*}
f(x_{1}, x_{2}) &= x_{1}^{3} + x_{1}^{2}x_{2} - x_{1}x_{2}^{2} + x_{2}^{3} \\[1em]
\frac{\partial f}{\partial x_{1}} &= 3x_{1}^{2} + 2x_{1}x_{2} - x_{2}^{2} \\[0.5em]
\frac{\partial f}{\partial x_{2}} &= x_{1}^{2} - 2x_{1}x_{2} + 3x_{2}^{2} \\[1em]
\frac{\partial^{2} f}{\partial x_{1}^{2}} &= 6x_{1} + 2x_{2} \\[0.5em]
\frac{\partial^{2} f}{\partial x_{1}\partial x_{2}} &= 2x_{1} - 2x_{2} \\[0.5em]
\frac{\partial^{2} f}{\partial x_{2}^{2}} &= -2x_{1} + 6x_{2} \\[1em]
\text{Hessian:}\quad
\nabla^{2} f(x_{1}, x_{2}) &=
\begin{bmatrix}
6x_{1} + 2x_{2} & 2x_{1} - 2x_{2} \\
2x_{1} - 2x_{2} & -2x_{1} + 6x_{2}
\end{bmatrix}
\end{align*}
$$
:::

## Multivariable Derivative Tests

For multivariate functions, convexity is determined by the Hessian matrix:

- If the Hessian $\nabla^2 f(\mathbf{x})$ is **positive semi-definite** for all $\mathbf{x}$, then $f$ is convex.
- If the Hessian is **negative semi-definite**, then $f$ is concave.

Convex functions are important in optimization because any local minimum is also a global minimum.

---

For multivariate functions, use the Hessian matrix:

- If the Hessian at $\mathbf{x}_0$ is **positive definite**, $f(\mathbf{x}_0)$ is a local minimum.
- If the Hessian is **negative definite**, $f(\mathbf{x}_0)$ is a local maximum.
- If the Hessian is **indefinite**, $\mathbf{x}_0$ is a saddle point.


## Vector-Valued Functions

A vector-valued function $\mathbf{f}: \mathbb{R}^{n}\rightarrow \mathbb{R}^{m}$ is a mapping from $n$-dimensional space to $m$-dimensional space

$$
\mathbf{f}(\mathbf{x}) = 
\begin{bmatrix}
f_{1}(\mathbf{x}) \\
f_{2}(\mathbf{x}) \\
\vdots \\
f_{m}(\mathbf{x}) \\
\end{bmatrix}\in\mathbb{R}^{m}.
$$

The rules for differentiation apply to each of the functions $f_{i}(\mathbf{x})$.

## Gradient of Vector-Valued Functions

The gradient of a vector-valued function $\mathbf{f}: \mathbb{R}^{n}\rightarrow \mathbb{R}^{m}$ is a matrix

$$
\small
J = 
\nabla \mathbf{f}(\mathbf{x})
=
\begin{bmatrix}
\frac{\partial f_{1}}{\partial x_{1}} & \frac{\partial f_{1}}{\partial x_{2}} & \cdots & \frac{\partial f_{1}}{\partial x_{n}} \\
\frac{\partial f_{2}}{\partial x_{1}} & \frac{\partial f_{2}}{\partial x_{2}} &\cdots & \frac{\partial f_{2}}{\partial x_{n}} \\
\vdots & \ddots & \ddots & \vdots\\
\frac{\partial f_{m}}{\partial x_{1}} &\frac{\partial f_{m}}{\partial x_{2}} & \cdots & \frac{\partial f_{m}}{\partial x_{n}}.
\end{bmatrix}\in\mathbb{R}^{m\times n}.
$$

This matrix is called the *Jacobian* and describes the rate of change of each output variable w.r.t. each input variable.

---

**Example:** $\mathbf{f}(x_{1},x_{2}) = \begin{bmatrix} \sin(x_{1}x_{2}) \\ 4x_{1}^{2}e^{x_{1}x_{2}} \end{bmatrix}$

::: {.notes}
$$
\begin{align*}
\mathbf{f}(x_{1},x_{2}) &= 
\begin{bmatrix}
\sin(x_{1}x_{2}) \\
4x_{1}^{2}e^{x_{1}x_{2}}
\end{bmatrix} \\[1em]
J = \nabla \mathbf{f}(x_{1},x_{2}) &=
\begin{bmatrix}
\frac{\partial}{\partial x_{1}}\sin(x_{1}x_{2}) & \frac{\partial}{\partial x_{2}}\sin(x_{1}x_{2}) \\[0.5em]
\frac{\partial}{\partial x_{1}}4x_{1}^{2}e^{x_{1}x_{2}} & \frac{\partial}{\partial x_{2}}4x_{1}^{2}e^{x_{1}x_{2}}
\end{bmatrix} \\[1em]
&=
\begin{bmatrix}
x_{2}\cos(x_{1}x_{2}) & x_{1}\cos(x_{1}x_{2}) \\[0.5em]
8x_{1}e^{x_{1}x_{2}} + 4x_{1}^{2}x_{2}e^{x_{1}x_{2}} & 4x_{1}^{3}e^{x_{1}x_{2}}
\end{bmatrix}
\end{align*}
$$
:::

## Gradient of $A\mathbf{x}$

Let $\mathbf{f}(\mathbf{x}) = A\mathbf{x}$, where $A\in\mathbb{R}^{m\times n}$ and $\mathbf{x}\in\mathbb{R}^{n}$. What is $\frac{d\mathbf{f}}{d\mathbf{x}}$?

## Jacobian Dimensions

Observe that

:::: {.incremental}
- Jacobian of a scalar-valued function $f(\mathbf{x})$ where $\mathbf{x}\in\mathbb{R}^{n}$ is a vector $\nabla f(\mathbf{x})\in\mathbb{R}^{n}$.
- Jacobian of a vector-valued function $\mathbf{f}(\mathbf{x})$ where $\mathbf{x}\in\mathbb{R}^{n}$ and $\mathbf{f}\in\mathbb{R}^{m}$ is a matrix $J\in\mathbb{R}^{m\times n}$.
- What is the Jacobian of a matrix-valued function?
::::


## Tensors

Tensors are a generalization of vectors and matrices to multidimensional arrays.

Tensors arise naturally in deep learning applications.

An $n$-dimensional tensor is an array 

$$
\mathcal{A}\in\mathbb{R}^{I_{1}\times I_{2}\times \cdots \times I_{n}}.
$$

---

An example of a 3rd order tensor is an RGB image which is an array

$$
\mathcal{A}\in\mathbb{R}^{m\times n \times 3}.
$$

## Gradients of Matrices

It is possible that we have a function mapping:

$$
\mathbf{f}: \mathbb{R}^{m\times n} \rightarrow \mathbb{R}^{p\times q}.
$$

For a linear mapping we can express this as the multiplication of $4$th order tensor $T\in\mathbb{R}^{p\times q\times m\times n}$ with the matrices $A$ and $B$,

$$
B_{ij} = \sum_{k=1}^{m}\sum_{l=1}^{n}T_{ijkl}A_{kl}.
$$

---

Alternatively, we could *flatten* the matrices, $A$ and $B$ so that they become vectors $\tilde{A}\in\mathbb{R}^{mn}$ and $\tilde{B}\in\mathbb{R}^{pq}$. The $4$th order tensor $T$ can also be flattened into a matrix $\tilde{T}\in\mathbb{R}^{pq\times mn}$ and we can compute

$$
\tilde{B} = \tilde{T}\tilde{A}.
$$

The matrix $\tilde{B}$ can be *unflattened* into a matrix of size $p\times q$.

---

Let $A\in\mathbb{R}^{m\times n}$ and $B\in\mathbb{R}^{p\times q}$. How can we compute the derivative $\frac{\partial A}{\partial B}$?

The result of this operation is a $4$-dimensional tensor

$$
J_{ijkl} = \frac{\partial A_{ij}}{\partial B_{kl}}\in\mathbb{R}^{m\times n \times p \times q}.
$$

## Gradient of Vector w.r.t. Matrix

Let's compute $\frac{d\mathbf{f}}{dA}\in\mathbb{R}^{m\times m\times n}$ when $\mathbf{f}=A\mathbf{x}\in\mathbb{R}^{m}$, where $A\in\mathbb{R}^{m\times n}$, and $\mathbf{x}\in\mathbb{R}^{n}$. 

$$
\frac{d\mathbf{f}}{dA}
=
\begin{bmatrix}
\frac{\partial f_{1}}{\partial A}\\
\frac{\partial f_{2}}{\partial A}\\
\vdots \\
\frac{\partial f_{m}}{\partial A}\\
\end{bmatrix}
$$

---

Compute $\frac{df_{i}}{dA_{ij}}$:

::: {.notes}
$$
\begin{align*}
\mathbf{f} &= A\mathbf{x} \\
f_{k} &= \sum_{l=1}^{n} A_{kl} x_{l} \\
\frac{\partial f_{k}}{\partial A_{ij}} &= \frac{\partial}{\partial A_{ij}} \left( \sum_{l=1}^{n} A_{kl} x_{l} \right) \\
&= \sum_{l=1}^{n} \frac{\partial A_{kl}}{\partial A_{ij}} x_{l} \\
&= \sum_{l=1}^{n} \delta_{ki}\delta_{lj} x_{l} \\
&= \delta_{ki} x_{j}
\end{align*}
$$
:::


---

Compute $\frac{df_{i}}{dA_{kj}}$:


## Gradient of a Matrix w.r.t. a Matrix

Let $R\in\mathbb{R}^{m\times n}$ and $K=R^{\top}R\in\mathbb{R}^{n\times n}$. What is $\frac{dK}{dR}$?


:::: {.fragment}
Compute $\frac{dK_{pq}}{dR_{ij}}$
::::


---

**Continued**



## Useful Identities for Computing Gradients

$$
\begin{align}
\frac{\partial A\mathbf{x}}{\partial \mathbf{x}} &= A, \quad
\frac{\partial \mathbf{x}^{\top}A}{\partial \mathbf{x}} = A^{\top}, \\
\frac{\partial \mathbf{x}^{\top}\mathbf{a}}{\partial \mathbf{x}} &= \mathbf{a}^{\top}, \quad
\frac{\partial \mathbf{a}^{\top}\mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}^{\top}, \\
\frac{\partial \mathbf{a}^{\top}X\mathbf{b}}{\partial X} &= \mathbf{a}\mathbf{b}^{\top}, \quad
\frac{\partial \mathbf{x}^{\top}A\mathbf{x}}{\partial \mathbf{x}} = \mathbf{x}^{\top}(A + A^{\top}). \\
\end{align}
$$

::: {.notes}
Numerator layout.
:::

## Automatic Differentiation

Automatic differentiation is a set of techniques to numerically evaluate the derivative of a function.

It is an efficient and accurate way to compute gradients by repeated use of the chain rule.

Recall that if we have a function $h(x) = f(g(x))$ then

$$
\frac{dh}{dx} = \frac{df}{dg}\frac{dg}{dx}.
$$

---

### Additive Rule

If $h(x) = u(f(x)) + v(g(x))$, then

$$
\frac{dh}{dx} = \frac{du}{df}\frac{df}{dx} + \frac{dv}{dg}\frac{dg}{dx}.
$$

---

Consider the function 

$$
f(x) = \sqrt{x^{2} + e^{x^{2}}} + \cos{(x^{2} + e^{x^{2}})}.
$$

The derivative is

$$
f^{\prime}(x) = 2x \left(\frac{1}{2\sqrt{x^{2}+e^{x^{2}}}} - \sin\left(x^{2} + e^{x^{2}}\right) \right) (1 + e^{x^{2}}).
$$

We'll use automatic differentiation to compute the derivative efficiently.

---

![](figures/ForwardPass.png){fig-align="center"}

How can we compute $\frac{df}{dx}$?

:::: {.columns}
::: {.column width="30%"}
$$
\small
\begin{align}
a &= x^{2} \\
b &= e^{x^{2}} \\
c &= a+b \\
d &= \sqrt{c} \\
e &= \cos{(c)} \\
f &= d+e \\ 
\end{align}
$$
:::
::: {.column width="70%"}

:::
::::

---

![](figures/NeuralNetwork.png){fig-align="center"}

---

The output of the neural network is

<!-- $$
\scriptsize
z^{[2]} = g^{[2]}(W^{[2]}\mathbf{a}^{[1]} + \mathbf{b}^{[2]}) = g^{[2]}(W^{[2]}g^{[1]}(W^{[1]}\mathbf{x} + \mathbf{b}^{[1]}) + b^{[2]}).
$$ -->

$$
\scriptsize
z^{[2]} = g^{[2]}(W^{[2]}\mathbf{z}^{[1]}) = g^{[2]}(W^{[2]}g^{[1]}(W^{[1]}\mathbf{x})).
$$

How do we compute the derivative of $z^{[2]}$ with respect to $\mathbf{x}$?

:::: {.columns}
::: {.column width="50%"}

$$
\small
\begin{align}
h^{[1]} &= W^{[1]}\mathbf{x} \\
\mathbf{z}^{[1]} &= g^{[1]}(h^{[1]})\\
h^{[2]} &= W^{[2]}\mathbf{z}^{[1]} \\
z^{[2]} &= g^{[2]}(h^{[2]})
\end{align}
$$

:::
::: {.column width="50%"}
:::: {.fragment}
$$
\scriptsize
\begin{align}
\frac{dh^{[1]}}{d\mathbf{x}} &= W^{[1]} \\
\frac{d\mathbf{z}^{[1]}}{dh^{[1]}} &= \frac{dg^{[1]}}{dh^{[1]}} \\
\frac{dh^{[2]}}{d\mathbf{z}^{[1]}} &= W^{[2]} \\
\frac{dz^{[2]}}{dh^{[2]}} &= \frac{dg^{[2]}}{dh^{[2]}} \\
\end{align}
$$
::::
:::
::::

## Multivariate Taylor Series

Let $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$ be $k$-times differentiable function at the point $\mathbf{c}\in\mathbb{R}^{n}$ then

$$
\scriptsize
f(\mathbf{x}) = f(\mathbf{c}) + \nabla f(\mathbf{c})^{\top}(\mathbf{x}-\mathbf{c}) + \frac{1}{2}(\mathbf{x}-\mathbf{c})^{\top}\nabla^{2}f(\mathbf{c})(\mathbf{x}-\mathbf{c}) + \mathcal{O}(\Vert \mathbf{x}-\mathbf{c}\Vert^{3}).
$$


## Summary

We covered:

- Gradients, Hessians, and Jacobians
- Tensors
- Gradients of vectors
- Gradients of matrices
- Automatic Differentiation
- Multivariate Taylor's Theorem